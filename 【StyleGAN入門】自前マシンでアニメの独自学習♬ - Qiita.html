<!DOCTYPE html><html><head><meta charset="utf-8" /><title>【StyleGAN入門】自前マシンでアニメの独自学習♬  - Qiita</title><meta content="width=device-width,initial-scale=1,shrink-to-fit=no" name="viewport" /><meta content="#55c500" name="theme-color" /><meta content="XWpkTG32-_C4joZoJ_UsmDUi-zaH-hcrjF6ZC_FoFbk" name="google-site-verification" /><link href="/manifest.json" rel="manifest" /><link href="/opensearch.xml" rel="search" title="Qiita" type="application/opensearchdescription+xml" /><meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="BtKSYp32Xu+sCrdnFQ7K8RagsDlqCjUZFpr8QXF5QoXfpOl1I/OrdoGoycBGZsLQZpve3m7ktCr0r3bRIyG46g==" /><link rel="shortcut icon" type="image/x-icon" href="https://cdn.qiita.com/assets/favicons/public/production-c620d3e403342b1022967ba5e3db1aaa.ico" /><link rel="apple-touch-icon" type="image/png" href="https://cdn.qiita.com/assets/favicons/public/apple-touch-icon-ec5ba42a24ae923f16825592efdc356f.png" /><link rel="stylesheet" media="all" href="https://cdn.qiita.com/assets/public/style-bb4a37b2fd2159ea75896f2f1cc75eb9.min.css" /><script src="https://cdn.qiita.com/assets/public/v3-article-bundle-942396f9c5bddb852b3c96a4e3220564.min.js" defer="defer"></script><meta name="twitter:card" content="summary_large_image"><meta content="@Qiita" name="twitter:site" /><meta property="og:type" content="article"><meta property="og:title" content="【StyleGAN入門】自前マシンでアニメの独自学習♬  - Qiita"><meta property="og:image" content="https://qiita-user-contents.imgix.net/https%3A%2F%2Fcdn.qiita.com%2Fassets%2Fpublic%2Farticle-ogp-background-1150d8b18a7c15795b701a55ae908f94.png?ixlib=rb-1.2.2&amp;w=1200&amp;mark=https%3A%2F%2Fqiita-user-contents.imgix.net%2F~text%3Fixlib%3Drb-1.2.2%26w%3D840%26h%3D380%26txt%3D%25E3%2580%2590StyleGAN%25E5%2585%25A5%25E9%2596%2580%25E3%2580%2591%25E8%2587%25AA%25E5%2589%258D%25E3%2583%259E%25E3%2582%25B7%25E3%2583%25B3%25E3%2581%25A7%25E3%2582%25A2%25E3%2583%258B%25E3%2583%25A1%25E3%2581%25AE%25E7%258B%25AC%25E8%2587%25AA%25E5%25AD%25A6%25E7%25BF%2592%25E2%2599%25AC%2520%26txt-color%3D%2523333%26txt-font%3DAvenir-Black%26txt-size%3D54%26txt-clip%3Dellipsis%26txt-align%3Dcenter%252Cmiddle%26s%3D6a7dc7ec42c3c13f01ed54a5d8138c48&amp;mark-align=center%2Cmiddle&amp;blend=https%3A%2F%2Fqiita-user-contents.imgix.net%2F~text%3Fixlib%3Drb-1.2.2%26w%3D840%26h%3D500%26txt%3D%2540MuAuan%26txt-color%3D%2523333%26txt-font%3DAvenir-Black%26txt-size%3D45%26txt-align%3Dright%252Cbottom%26s%3Dcce244377480debd5de884488ed379f2&amp;blend-align=center%2Cmiddle&amp;blend-mode=normal&amp;s=a0d629df835598eaaf3dacda3e1f2c1e"><meta property="og:description" content="今回はStyleGANを使ってアニメ顔の学習に挑戦してみました。
学習に関する参考はほとんどなく以下のものがありました。結構学習に時間もかかるし、情報も不十分でしたが、一応自前マシンで学習でき、かつ学習途中からの再学習もできたので、記..."><meta content="https://qiita.com/MuAuan/items/aec7feabaa2f738ea82c" property="og:url" /><meta content="Qiita" property="og:site_name" /><meta content="564524038" property="fb:admins" /><meta content="Python,機械学習,DeepLearning,動画,stylegan" name="keywords" /><style data-styled="true" data-styled-version="5.0.1">.iytOeR{display:inline-block;border:2px solid #ccc;font-size:1.4rem;padding:4px 0;text-align:center;cursor:pointer;width:100%;background-color:#fff;color:#333;border-color:#ddd;font-weight:bold;}
.iytOeR:hover{border-color:#adadad;background-color:#e6e6e6;color:#333;}
data-styled.g1[id="UserFollowButton-sc-1hmm0rc-0"]{content:"iytOeR,"}
.fsja-Dh > .banReason{list-style:disc;list-style-position:inside;margin:10px 0 10px;}
.fsja-Dh > .banReason > li{padding-left:12px;font-weight:bold;}
.fsja-Dh > .banReasonBody{font-size:15px;margin-bottom:10px;}
data-styled.g3[id="ArticleAlert-sc-14a2ewm-0"]{content:"fsja-Dh,"}
.ZHnTl{display:inline-block;width:56px;height:12.747967479674797px;fill:#55C500;}
data-styled.g4[id="LgtmIcon__Lgtm-sc-1e4ee48-0"]{content:"ZHnTl,"}
.pFoNU{display:inline-block;vertical-align:middle;height:12px;width:12px;fill:#777;}
data-styled.g5[id="VerticalLgtmIcon__Lgtm-sc-19v9h1n-0"]{content:"pFoNU,"}
.fEysOa{display:inline-block;vertical-align:bottom;height:17.77777777777778px;width:16px;fill:#5D707C;}
data-styled.g6[id="StockIcon__Stock-cq5opj-0"]{content:"fEysOa,"}
.hGIfZz{display:inline-block;border-radius:3px;line-height:1;overflow:hidden;vertical-align:middle;}
data-styled.g10[id="Avatar__Image-sc-1u4xwgc-0"]{content:"hGIfZz,"}
.hEwpMS{display:inline-block;box-sizing:border-box;min-width:64px;padding:6px 16px;font-size:14px;font-weight:400;height:34px;margin-bottom:0;text-align:center;white-space:nowrap;vertical-align:middle;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;border:1px solid transparent;border-radius:0.2em;outline:none;color:#fff;background-color:#59bb0c;border-color:#4ea30a;}
.hEwpMS:hover{background-color:#428b09;border-color:#326a07;}
.hEwpMS:disabled{cursor:not-allowed;opacity:0.65;box-shadow:none;background-color:#59bb0c;border-color:#4ea30a;}
data-styled.g11[id="Button-sc-998iob-0"]{content:"hEwpMS,"}
.gnNGXY{margin-bottom:10px;text-align:left;}
data-styled.g18[id="CommentForm__Header-xcwl8q-0"]{content:"gnNGXY,"}
.eZJAwV{display:inline-block;line-height:36px;size:15px;font-size:15px;font-weight:bold;margin-top:0;margin-left:10px;}
data-styled.g19[id="CommentForm__HeaderTitle-xcwl8q-1"]{content:"eZJAwV,"}
.fZEEYf{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;border-top-left-radius:3px;border-top-right-radius:3px;background-color:#fafafa;border:1px solid #ccc;border-bottom:0;}
data-styled.g20[id="CommentForm__ContentHeader-xcwl8q-2"]{content:"fZEEYf,"}
.lnidcq{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}
data-styled.g21[id="CommentForm__ContentHeaderTabs-xcwl8q-3"]{content:"lnidcq,"}
.esvsAC{padding:10px 20px;font-size:13px;color:#666;background-color:#fff;cursor:pointer;line-height:20px;}
.esvsAC hover{-webkit-text-decoration:none;text-decoration:none;}
.joOXXL{padding:10px 20px;font-size:13px;color:#aaa;cursor:pointer;line-height:20px;}
.joOXXL hover{-webkit-text-decoration:none;text-decoration:none;}
data-styled.g22[id="CommentForm__ContentHeaderTabsItem-xcwl8q-4"]{content:"esvsAC,joOXXL,"}
.jkYbvp{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}
data-styled.g23[id="CommentForm__ContentHeaderToolbar-xcwl8q-5"]{content:"jkYbvp,"}
.eYFchP{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;cursor:pointer;width:32px;height:32px;color:#aaa;margin-right:10px;}
.eYFchP:hover{color:#666;}
data-styled.g24[id="CommentForm__ContentHeaderToolbarItem-xcwl8q-6"]{content:"eYFchP,"}
.QXFlv{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;cursor:pointer;width:32px;height:32px;color:#aaa;margin-right:10px;}
.QXFlv:hover{color:#666;-webkit-text-decoration:none;text-decoration:none;}
data-styled.g25[id="CommentForm__ContentHeaderToolbarLink-xcwl8q-7"]{content:"QXFlv,"}
.dkpzIG{padding:5px 5px 0;border:1px solid #ccc;border-top:0;border-bottom:0;background-color:#fff;}
data-styled.g28[id="CommentForm__Body-xcwl8q-10"]{content:"dkpzIG,"}
.drBRWd{padding:18px;min-height:100px;background-color:#fff;}
data-styled.g29[id="CommentForm__BodyPreview-xcwl8q-11"]{content:"drBRWd,"}
.fEfHfr{border-radius:0;font-size:14px;-webkit-transition:background-color .1s linear;transition:background-color .1s linear;min-height:100px;border:1px solid #ddd;box-shadow:inset 0 1px 3px #eee;background-color:#fafafa;padding:10px;color:#333;resize:vertical;width:100%;}
data-styled.g30[id="CommentForm__CommentFormTextarea-xcwl8q-12"]{content:"fEfHfr,"}
.XemJY{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;border:1px solid #ccc;border-top:0;padding:6px;min-height:46px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}
data-styled.g31[id="CommentForm__Footer-xcwl8q-13"]{content:"XemJY,"}
.hEjQqy{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
.hEjQqy > *{margin-right:10px;}
data-styled.g32[id="CommentForm__Uploader-xcwl8q-14"]{content:"hEjQqy,"}
.hZWjpt{display:inline-block;color:#555;background-color:#fff;padding:6px 10px;font-size:13px;vertical-align:baseline;line-height:1;cursor:pointer;touch-action:manipulation;border-radius:3px;border:1px solid #ccc;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;}
.hZWjpt:hover{background-color:#e6e6e6;border-color:#adadad;}
.hZWjpt > i{margin-right:8px;}
data-styled.g33[id="CommentForm__UploaderButton-xcwl8q-15"]{content:"hZWjpt,"}
.bMzayN{display:inline-block;color:rgb(153,153,153);}
data-styled.g34[id="CommentForm__UploaderText-xcwl8q-16"]{content:"bMzayN,"}
</style><script>window.frtn=window.frtn||function(){ (frtn.q=frtn.q||[]).push(arguments) };
frtn("init",{
service_id:"cova_248",
site_id:"site_134",
tag_id:"tag_283"
});
frtn("send","pageview");</script><script defer="" src="https://frtn.socdm.com/tags/insight.js" type="text/javascript"></script><script>!function(t,e){if(void 0===e[t]){e[t]=function(){e[t].clients.push(this),this._init=[Array.prototype.slice.call(arguments)]},e[t].clients=[];for(var r=function(t){return function(){return this["_"+t]=this["_"+t]||[],this["_"+t].push(Array.prototype.slice.call(arguments)),this}},s=["blockEvents","unblockEvents","setSignedMode","setAnonymousMode","resetUUID","addRecord","fetchGlobalID","set","trackEvent","trackPageview","trackClicks","ready"],n=0;n<s.length;n++){var c=s[n];e[t].prototype[c]=r(c)}var o=document.createElement("script");o.type="text/javascript",o.async=!0,o.src=("https:"===document.location.protocol?"https:":"http:")+"//cdn.treasuredata.com/sdk/2.1/td.min.js";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(o,a)}}("Treasure",this);

// Configure an instance for your database
var td = new Treasure({
  host: 'in.treasuredata.com',
  writeKey: '10614/f5a4453704ad12facc47fe5281fd57526a6119e9',
  database: 'qiita_public',
  startInSignedMode: true
});

// Enable cross-domain tracking
td.set('$global', 'td_global_id', 'td_global_id');
// Track pageview information to 'pageviews' table
td.set('pageviews_all', {"query_parameters":{},"path_parameters":{"controller":"public/items","action":"show","user_id":"MuAuan","type":"items","id":"aec7feabaa2f738ea82c"},"user_id":595703})
td.trackPageview('pageviews_all');</script></head><body><div class="allWrapper"><div class="st-HeaderContainer"><div id="GlobalHeader-react-component-a1cd4f44-4052-4688-a01c-c1c344974c89"><div class="st-Header"><div class="st-Header_container"><div class="st-Header_start"><a href="/" class="st-Header_logo mr-1"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 426.57 130"><circle cx="167.08" cy="21.4" r="12.28"></circle><path d="M250.81 29.66h23.48v18.9h-23.48z"></path><path d="M300.76 105.26a22.23 22.23 0 01-6.26-.86 12.68 12.68 0 01-5.17-3 14.41 14.41 0 01-3.56-5.76 28 28 0 01-1.3-9.22V48.56h29.61v-18.9h-29.52V3.29h-20.17v83.34q0 11.16 2.83 18.27a27.71 27.71 0 007.7 11.2 26.86 26.86 0 0011.43 5.62 47.56 47.56 0 0012.34 1.53h15.16v-18zM0 61.7a58.6 58.6 0 015-24.21A62.26 62.26 0 0118.73 17.9 63.72 63.72 0 0139 4.78 64.93 64.93 0 0164 0a65 65 0 0124.85 4.78 64.24 64.24 0 0120.38 13.12A62 62 0 01123 37.49a58.6 58.6 0 015 24.21 58.34 58.34 0 01-4 21.46 62.8 62.8 0 01-10.91 18.16l11.1 11.1a10.3 10.3 0 010 14.52 10.29 10.29 0 01-14.64 0l-12.22-12.41a65 65 0 01-15.78 6.65 66.32 66.32 0 01-17.55 2.3 64.63 64.63 0 01-45.23-18A62.82 62.82 0 015 85.81 58.3 58.3 0 010 61.7zm21.64.08a43.13 43.13 0 0012.42 30.63 42.23 42.23 0 0013.43 9.09A41.31 41.31 0 0064 104.8a42 42 0 0030-12.39 42.37 42.37 0 009-13.64 43.43 43.43 0 003.3-17 43.77 43.77 0 00-3.3-17A41.7 41.7 0 0080.55 22 41.78 41.78 0 0064 18.68 41.31 41.31 0 0047.49 22a42.37 42.37 0 00-13.43 9.08 43.37 43.37 0 00-12.42 30.7zM331.89 78a47.59 47.59 0 013.3-17.73 43.22 43.22 0 019.34-14.47A44.25 44.25 0 01359 36a47.82 47.82 0 0118.81-3.58 42.72 42.72 0 019.26 1 46.5 46.5 0 018.22 2.58 40 40 0 017 3.84 44.39 44.39 0 015.71 4.63l1.22-9.47h17.35v85.83h-17.35l-1.17-9.42a42.54 42.54 0 01-5.84 4.67 43.11 43.11 0 01-7 3.79 44.86 44.86 0 01-8.17 2.59 43 43 0 01-9.22 1A47.94 47.94 0 01359 119.9a43.3 43.3 0 01-14.47-9.71 44.17 44.17 0 01-9.34-14.47 47 47 0 01-3.3-17.72zm20.27-.08a29.16 29.16 0 002.17 11.34 27 27 0 005.92 8.88 26.69 26.69 0 008.76 5.76 29.19 29.19 0 0021.44 0 26.11 26.11 0 008.72-5.76 27.57 27.57 0 005.88-8.84 29 29 0 002.16-11.38 28.62 28.62 0 00-2.16-11.22 26.57 26.57 0 00-5.93-8.8 27.68 27.68 0 00-19.51-7.9 28.29 28.29 0 00-10.77 2.05 26.19 26.19 0 00-8.71 5.75 27.08 27.08 0 00-5.84 8.8 28.94 28.94 0 00-2.13 11.31zm-194.97-30.5h19.78v73.54h-19.78zm49.25 0h19.78v73.54h-19.78z"></path><circle cx="216.33" cy="21.4" r="12.28"></circle></svg></a><div><div class="st-Header_realmSelector" tabindex="0"><span class="fa fa-fw fa-caret-down"></span></div><div class="st-Header_dropdown st-RealmSelector"><div class="st-RealmSelector_realms"><a class="st-Header_dropdownItem st-RealmItem" href="https://qiita.com/"><div class="st-RealmItem_statusIcon"><span class="fa fa-fw fa-check"></span></div><div class="st-RealmItem_humanName">Qiita</div></a></div><hr/><div class="st-RealmSelector_supplements"><div class="st-RealmSelector_label">ログイン中のチームがありません</div><a href="https://teams-center.qiita.com/find_team" class="st-Header_dropdownItem st-RealmSelectorSupplement"><div class="st-RealmSelectorSupplement_icon"><span class="fa fa-fw fa-sign-in"></span></div><div>Qiita Team にログイン...</div></a></div></div></div><div><div class="st-Header_community" tabindex="0">コミュニティ<span class="fa fa-fw fa-caret-down ml-1of2"></span></div><div class="st-Header_dropdown"><a href="/users" class="st-Header_dropdownItem"><span class="fa fa-fw fa-users mr-1of2"></span>ユーザー一覧</a><a href="/organizations" class="st-Header_dropdownItem"><span class="fa fa-fw fa-building-o mr-1of2"></span>Organization一覧</a><a href="/advent-calendar" class="st-Header_dropdownItem"><span class="fa fa-fw fa-calendar mr-1of2"></span>アドベントカレンダー</a><div class="st-Header_dropdownSeparator"></div><a href="https://jobs.qiita.com/?utm_source=qiita&amp;utm_medium=referral&amp;utm_content=header" class="st-Header_dropdownItem" target="_blank"><span class="fa fa-fw fa-search mr-1of2"></span>Qiita Jobs</a><a href="https://qiitadon.com/" class="st-Header_dropdownItem" target="_blank"><span class="fa fa-fw fa-comments-o mr-1of2"></span>Qiitadon (β)</a><a href="https://zine.qiita.com/?utm_source=qiita&amp;utm_medium=referral&amp;utm_content=header" class="st-Header_dropdownItem" target="_blank"><span class="fa fa-fw fa-newspaper-o mr-1of2"></span>Qiita Zine</a><div class="st-Header_dropdownSeparator"></div><a href="https://help.qiita.com/ja/articles/qiita-community-guideline" class="st-Header_dropdownItem" target="_blank"><span class="fa fa-fw fa-book mr-1of2"></span>コミュニティガイドライン</a><a href="https://help.qiita.com/ja/articles/qiita-article-guideline" class="st-Header_dropdownItem" target="_blank"><span class="fa fa-fw fa-book mr-1of2"></span>良い記事を書くために</a></div></div><form class="st-Header_search" action="/search" method="get"><span class="fa fa-search"></span><input type="search" class="st-Header_searchInput" autoComplete="off" placeholder="キーワードを入力" value="" name="q" required=""/></form><div class="st-Header_searchButton"><span class="fa fa-search"></span></div></div><div class="st-Header_end"><a href="/stock" class="st-Header_stock mr-2 py-2"><span class="st-Header_stock-icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 359.16 339" color="#fff" style="fill:#fff;width:13px;height:14.3px;display:inline-block;vertical-align:bottom"><path d="M359.16 251.41V0H0v251.41c91.24 116.79 267.92 116.79 359.16 0zm-53.22-131.26L187.73 253a23 23 0 01-16.23 7.69h-.94a23 23 0 01-15.87-6.35L54.53 158.86a23 23 0 0131.73-33.27l82.94 79.11L271.59 89.6a23 23 0 1134.35 30.55z"></path></svg></span><span class="st-Header_stock-label">ストック一覧</span></a><a href="/drafts/new" class="st-Header_postButton mr-2 px-2"><span class="fa fa-fw fa-pencil-square-o mr-1"></span>投稿する</a><div class="u-relative"><div class="st-Header_notifications" tabindex="0">0</div><div class="st-Header_notiIframe"><div class="st-Header_notiHeader"><div><span class="fa fa-bell-o mgr-1"></span>お知らせ</div><a href="/settings/notifications">通知設定</a></div><div class="st-Header_notiFooter"><a href="/notifications">通知一覧を見る</a></div></div></div><div class="u-relative"><div class="st-Header_loginUser" tabindex="0"><img src="https://qiita-user-profile-images.imgix.net/https%3A%2F%2Flh6.googleusercontent.com%2F-3EHSHCCSCmg%2FAAAAAAAAAAI%2FAAAAAAAAAAA%2FAKF05nAVp3wix0GPOS2jqZ7xSDRYCDF6Mw%2Fs50%2Fphoto.jpg?ixlib=rb-1.2.2&amp;auto=compress%2Cformat&amp;lossless=0&amp;w=48&amp;s=be4e5cacc9e0864ac1ef96d37914a7b9" alt="syoborian"/><span class="fa fa-fw fa-caret-down ml-1"></span></div><div class="st-Header_dropdown"><a href="/syoborian" class="st-Header_dropdownItem">マイページ</a><div class="st-Header_dropdownSeparator"></div><a href="/drafts" class="st-Header_dropdownItem">下書き一覧</a><a href="/syoborian/private" class="st-Header_dropdownItem">限定共有記事一覧</a><a href="/patches" class="st-Header_dropdownItem">編集リクエスト一覧</a><div class="st-Header_dropdownSeparator"></div><a href="/settings" class="st-Header_dropdownItem">設定</a><a href="https://help.qiita.com/ja/categories/qiita/" class="st-Header_dropdownItem" target="_blank">ヘルプ</a><div class="st-Header_dropdownSeparator"></div><div class="st-Header_dropdownItem" tabindex="0">ログアウト</div></div></div></div><div class="st-Header_overlay"></div><form class="st-Header_searchModal" action="/search" method="get"><input type="text" class="st-Header_searchModalInput" autoComplete="off" placeholder="キーワードを入力" value="" name="q" required=""/></form></div></div></div>
      <script type="application/json" class="js-react-on-rails-component" data-component-name="GlobalHeader" data-dom-id="GlobalHeader-react-component-a1cd4f44-4052-4688-a01c-c1c344974c89">{"unreadNotificationsCount":0,"realms":[{"humanName":"Qiita","isCurrentRealm":true,"isQiita":true,"isQiitaTeam":false,"loggedInUser":{"profileImageUrl":"https://lh6.googleusercontent.com/-3EHSHCCSCmg/AAAAAAAAAAI/AAAAAAAAAAA/AKF05nAVp3wix0GPOS2jqZ7xSDRYCDF6Mw/s50/photo.jpg","urlName":"syoborian"},"teamId":null,"url":"https://qiita.com/"}],"teamFindUrl":"https://teams-center.qiita.com/find_team","isTeamOnlyUser":false,"currentUser":{"isStaff":false,"isJobseeker":false,"name":"","originalId":595703,"profileImageUrl":"https://qiita-user-profile-images.imgix.net/https%3A%2F%2Flh6.googleusercontent.com%2F-3EHSHCCSCmg%2FAAAAAAAAAAI%2FAAAAAAAAAAA%2FAKF05nAVp3wix0GPOS2jqZ7xSDRYCDF6Mw%2Fs50%2Fphoto.jpg?ixlib=rb-1.2.2\u0026auto=compress%2Cformat\u0026lossless=0\u0026w=48\u0026s=be4e5cacc9e0864ac1ef96d37914a7b9","urlName":"syoborian"}}</script>
      
</div><div class="st-HeaderAlert st-HeaderAlert-warning"><div class="st-HeaderAlert_body"></div></div><script type="application/ld+json">{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"/","name":"Qiita"}},{"@type":"ListItem","position":2,"item":{"@id":"/tags/%23%3CQiita::Graph::Result:0x00005594c8dc74d8%3E","name":"Python"}}]}</script><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","datePublished":"2020-01-23T23:28:45.000+09:00","dateModified":"2020-01-24T07:36:56.000+09:00","headline":"【StyleGAN入門】自前マシンでアニメの独自学習♬ ","image":"https://qiita-user-contents.imgix.net/https%3A%2F%2Fcdn.qiita.com%2Fassets%2Fpublic%2Farticle-ogp-background-1150d8b18a7c15795b701a55ae908f94.png?ixlib=rb-1.2.2\u0026w=1200\u0026mark=https%3A%2F%2Fqiita-user-contents.imgix.net%2F~text%3Fixlib%3Drb-1.2.2%26w%3D840%26h%3D380%26txt%3D%25E3%2580%2590StyleGAN%25E5%2585%25A5%25E9%2596%2580%25E3%2580%2591%25E8%2587%25AA%25E5%2589%258D%25E3%2583%259E%25E3%2582%25B7%25E3%2583%25B3%25E3%2581%25A7%25E3%2582%25A2%25E3%2583%258B%25E3%2583%25A1%25E3%2581%25AE%25E7%258B%25AC%25E8%2587%25AA%25E5%25AD%25A6%25E7%25BF%2592%25E2%2599%25AC%2520%26txt-color%3D%2523333%26txt-font%3DAvenir-Black%26txt-size%3D54%26txt-clip%3Dellipsis%26txt-align%3Dcenter%252Cmiddle%26s%3D6a7dc7ec42c3c13f01ed54a5d8138c48\u0026mark-align=center%2Cmiddle\u0026blend=https%3A%2F%2Fqiita-user-contents.imgix.net%2F~text%3Fixlib%3Drb-1.2.2%26w%3D840%26h%3D500%26txt%3D%2540MuAuan%26txt-color%3D%2523333%26txt-font%3DAvenir-Black%26txt-size%3D45%26txt-align%3Dright%252Cbottom%26s%3Dcce244377480debd5de884488ed379f2\u0026blend-align=center%2Cmiddle\u0026blend-mode=normal\u0026s=a0d629df835598eaaf3dacda3e1f2c1e","mainEntityOfPage":"https://qiita.com/MuAuan/items/aec7feabaa2f738ea82c","author":{"@type":"Person","address":"お花茶屋","email":null,"identifier":"MuAuan","name":"MuAuan","image":"https://qiita-user-profile-images.imgix.net/https%3A%2F%2Favatars2.githubusercontent.com%2Fu%2F35788955%3Fv%3D4?ixlib=rb-1.2.2\u0026auto=compress%2Cformat\u0026lossless=0\u0026w=75\u0026s=e420ddb9abfbecae939e655562343f59","url":"https://qiita.com/MuAuan","description":"2020年目標；いい記事を書く\r\n\r\n記事350いいね1500フォロワー150\r\n2019年の実績／目標\r\n記事275／300いいね1035／1000フォロワー97／100\r\n1/7/2019\r\n記事219いいね784フォロワー76\r\n2018年の実績／目標\r\n記事140／200いいね423／500フォロワー48／50\r\n7/8/2018\r\n記事90いいね227フォロワー25","memberOf":[]},"publisher":{"@type":"Organization","name":"Qiita","logo":{"@type":"ImageObject","url":"//cdn.qiita.com/assets/public/qiita-logo-green-36e6153054916b1d8ed7fc47163039da.png"}}}</script><script>td.trackEvent('pageviews_article',{
    user_id: "595703",
    article_id: "1144200",
    article_uuid: "aec7feabaa2f738ea82c",
    td_description: "&quot;今回はStyleGANを使ってアニメ顔の学習に挑戦してみました。\n学習に関する参考はほとんどなく以下のものがありました。結構学習に時間もかかるし、情報も不十分でしたが、一応自前マシンで学習でき、かつ学習途中からの再学習もできたので、記事にまとめておきます。\n【参考】\n①[How To Use Custom Datasets With StyleGAN - TensorFlow Implementation](https://evigio.com/post/how-to-use-custom-datasets-with-stylegan-tensorFlow-implementation)\n②[styleganで独自モデルの学習方法](http://blog.livedoor.jp/tak_tak0/archives/52409271.html)\n③[StyleGAN log](https://gist.github.com/eukaryote31/e7406e49f62f23e4004afa9788cafaa0)\n④[Making Anime Faces With StyleGAN](https://www.gwern.net/Faces)\n###やったこと\n・アニメ顔データの準備\n・とにかく学習する\n・潜在空間でのミキシングをやってみる\n・再学習するには\n###・アニメ顔データの準備\nアニメ顔は[以前DCGANで利用したサイト](https://qiita.com/MuAuan/items/85db7176574bdf979061)から、ダウンロードして準備しました。\n今回の利用のポイントは少なくとも画像サイズを合わせて、かつファイル名を変更して読み取りやすく、1.pngのように変更すること。\nStyleGANの学習という意味では目鼻顔などのアラインメントを合わせたいが、次回にパスした。\nということで、上記のデータ整理は以下のコードで実施した。\nちなみにサイズ（128,128）を1000個用意した。\n\n```py\nfrom PIL import Image\nimport glob\nimport random\n\nfiles = glob.glob(\u0026amp;quot;./anime/**/*.png\u0026amp;quot;, recursive=True)\nfiles = random.sample(files, 1000)\nres = []\nsk=0\nfor path in files:\n    img = Image.open(path)\n    img = img.resize((128, 128))\n    img.save( \u0026amp;quot;img/{}.png\u0026amp;quot;.format(sk))\n    sk += 1\n```\n###・とにかく学習する\n学習コードは、参考①のビデオと参考②を眺めつつ、以下のようにした。\n\nkimgは学習image数であり、単位が千imgを意味している。\n総学習イメージ数が3400kimgを意味する\n次の行は学習開始解像度＝４\nそして、custom_datasetがtf_recordsに変換した画像のDir(＝datasets/custom_dataset)である。さらに、とりあえずの学習ということで、解像度=64で学習してみたが問題なく学習できた。\nまた、StyleGANはpganであり、サイズ毎学習であるが、サイズ毎のminibatchサイズも以下のとおり、小さめに設定した。\n\n```py\ntrain.total_kimg = 3400、\nsched.lod_initial_resolution = 4\ndesc += \u0026amp;#39;-custom_dataset\u0026amp;#39;;     dataset = EasyDict(tfrecord_dir=\u0026amp;#39;custom_dataset\u0026amp;#39;, resolution=64);                 train.mirror_augment = False\ndesc += \u0026amp;#39;-1gpu\u0026amp;#39;; submit_config.num_gpus = 1; sched.minibatch_base = 4; sched.minibatch_dict = {4: 128, 8: 64, 16: 32, 32: 16, 64: 8, 128: 8, 256: 4, 512: 2}\n```\nつまり、以下の必要最小限で動く。\n\n```train.py\n# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n\u0026amp;quot;\u0026amp;quot;\u0026amp;quot;Main entry point for training StyleGAN and ProGAN networks.\u0026amp;quot;\u0026amp;quot;\u0026amp;quot;\n\nimport copy\nimport dnnlib\nfrom dnnlib import EasyDict\n\nimport config\nfrom metrics import metric_base\n\n#----------------------------------------------------------------------------\n# Official training configs for StyleGAN, targeted mainly for FFHQ.\n\nif 1:\n    desc          = \u0026amp;#39;sgan\u0026amp;#39;                                                                 # Description string included in result subdir name.\n    train         = EasyDict(run_func_name=\u0026amp;#39;training.training_loop.training_loop\u0026amp;#39;)         # Options for training loop.\n    G             = EasyDict(func_name=\u0026amp;#39;training.networks_stylegan.G_style\u0026amp;#39;)               # Options for generator network.\n    D             = EasyDict(func_name=\u0026amp;#39;training.networks_stylegan.D_basic\u0026amp;#39;)               # Options for discriminator network.\n    G_opt         = EasyDict(beta1=0.0, beta2=0.99, epsilon=1e-8)                          # Options for generator optimizer.\n    D_opt         = EasyDict(beta1=0.0, beta2=0.99, epsilon=1e-8)                          # Options for discriminator optimizer.\n    G_loss        = EasyDict(func_name=\u0026amp;#39;training.loss.G_logistic_nonsaturating\u0026amp;#39;)           # Options for generator loss.\n    D_loss        = EasyDict(func_name=\u0026amp;#39;training.loss.D_logistic_simplegp\u0026amp;#39;, r1_gamma=10.0) # Options for discriminator loss.\n    dataset       = EasyDict()                                                             # Options for load_dataset().\n    sched         = EasyDict()                                                             # Options for TrainingSchedule.\n    grid          = EasyDict(size=\u0026amp;#39;4k\u0026amp;#39;, layout=\u0026amp;#39;random\u0026amp;#39;)  #4k                              # Options for setup_snapshot_image_grid().\n    metrics       = [metric_base.fid50k]                                                   # Options for MetricGroup.\n    submit_config = dnnlib.SubmitConfig()                                                  # Options for dnnlib.submit_run().\n    tf_config     = {\u0026amp;#39;rnd.np_random_seed\u0026amp;#39;: 1000}                                           # Options for tflib.init_tf().\n\n    # Dataset.\n    desc += \u0026amp;#39;-custom_dataset\u0026amp;#39;;     dataset = EasyDict(tfrecord_dir=\u0026amp;#39;custom_dataset\u0026amp;#39;, resolution=64);                 train.mirror_augment = False\n\n    # Number of GPUs.\n    desc += \u0026amp;#39;-1gpu\u0026amp;#39;; submit_config.num_gpus = 1; sched.minibatch_base = 4; sched.minibatch_dict = {4: 128, 8: 64, 16: 32, 32: 16, 64: 8, 128: 8, 256: 4, 512: 2}\n\n    # Default options.\n    train.total_kimg = 3400\n    sched.lod_initial_resolution = 4\n    sched.G_lrate_dict = {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003}\n    sched.D_lrate_dict = EasyDict(sched.G_lrate_dict)\n\n#----------------------------------------------------------------------------\n# Main entry point for training.\n# Calls the function indicated by \u0026amp;#39;train\u0026amp;#39; using the selected options.\n\ndef main():\n    kwargs = EasyDict(train)\n    kwargs.update(G_args=G, D_args=D, G_opt_args=G_opt, D_opt_args=D_opt, G_loss_args=G_loss, D_loss_args=D_loss)\n    kwargs.update(dataset_args=dataset, sched_args=sched, grid_args=grid, metric_arg_list=metrics, tf_config=tf_config)\n    kwargs.submit_config = copy.deepcopy(submit_config)\n    kwargs.submit_config.run_dir_root = dnnlib.submission.submit.get_template_from_path(config.result_dir)\n    kwargs.submit_config.run_dir_ignore += config.run_dir_ignore\n    kwargs.submit_config.run_desc = desc\n    dnnlib.submit_run(**kwargs)\n\n#----------------------------------------------------------------------------\n\nif __name__ == \u0026amp;quot;__main__\u0026amp;quot;:\n    main()\n\n#----------------------------------------------------------------------------\n```\nなお、データのtfrecordsへの変換は参考①から以下のように実施した。\n※ここで元画像は./animeに入れておく、そして画像サイズ毎の変換ファイルはcustom_datasetに６個のサイズの異なるファイルが格納された。\n\n```\npython dataset_tool.py create_from_images datasets/custom_dataset ./anime\n```\n上記でとりあえず、学習できると思う。\n###・潜在空間でのミキシングをやってみる\n1060マシンで上記のコードを10h程度で以下の絵が得られる。\n決して綺麗とは言えないが、とにかく最弱マシンでも学習できた。\n![64x64_3400.jpg](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/233744/5a66106f-1906-e5e3-718d-402b320f922b.jpeg)\nさらに、潜在空間での17，18のミキシングをやってみると以下の絵が得られた。\n![example17_18.gif](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/233744/99bb4f44-d8d1-6ca7-aa65-08f087cf9601.gif)\n1080マシンで解像度128x128サイズで1d8h程度回して、kimg=4705の場合以下のように画像がしっかりしてきた。\n※これでもpid50K=168程度で精度はまだまだだが、。。。[以前のDCGANの画像](https://qiita.com/MuAuan/items/85db7176574bdf979061)と比べてこちらの方が綺麗に見える\n![128x128_4705_25.jpg](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/233744/6ce25027-b303-4de4-83fc-3909c57bf178.jpeg)\nさらに、潜在空間での11，82のミキシングをやってみると以下の絵が得られた。\n![example_128_4705_100_82x11.gif](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/233744/93df08de-f7a7-25d7-90ca-91a248fd84f2.gif)\n###・再学習するには\n最後に禁断（誰も公開していないようなので）の途中中断した場合の継続学習の仕方が出来たので、まとめておく。\n※この方法はちょっとだけ参考②と参考④に記載がある\n以下のコードで実施する。\nすなわち、training_loop.pyの以下の部分を修正する。\n\n```py\n    resume_run_id           = \u0026amp;quot;latest\u0026amp;quot;, #None,     # Run ID or network pkl to resume training from, None = start from scratch.\n    resume_snapshot         = \u0026amp;#39;./results/00001-sgan-custom_dataset-1gpu/network-snapshot-.pkl\u0026amp;#39;, #None,     # Snapshot index to resume training from, None = autodetect.\n```\nまた、network_snapshot_ticks  = 1,       # How often to export network snapshots? として、毎回出力にしている。\n\n```training_loop.py\ndef training_loop(\n    submit_config,\n    G_args                  = {},       # Options for generator network.\n    D_args                  = {},       # Options for discriminator network.\n    G_opt_args              = {},       # Options for generator optimizer.\n    D_opt_args              = {},       # Options for discriminator optimizer.\n    G_loss_args             = {},       # Options for generator loss.\n    D_loss_args             = {},       # Options for discriminator loss.\n    dataset_args            = {},       # Options for dataset.load_dataset().\n    sched_args              = {},       # Options for train.TrainingSchedule.\n    grid_args               = {},       # Options for train.setup_snapshot_image_grid().\n    metric_arg_list         = [],       # Options for MetricGroup.\n    tf_config               = {},       # Options for tflib.init_tf().\n    G_smoothing_kimg        = 10.0,     # Half-life of the running average of generator weights.\n    D_repeats               = 1,        # How many times the discriminator is trained per G iteration.\n    minibatch_repeats       = 4,        # Number of minibatches to run before adjusting training parameters.\n    reset_opt_for_new_lod   = True,     # Reset optimizer internal state (e.g. Adam moments) when new layers are introduced?\n    total_kimg              = 15000,    # Total length of the training, measured in thousands of real images.\n    mirror_augment          = False,    # Enable mirror augment?\n    drange_net              = [-1,1],   # Dynamic range used when feeding image data to the networks.\n    image_snapshot_ticks    = 1,        # How often to export image snapshots?\n    network_snapshot_ticks  = 1,       # How often to export network snapshots? default=10\n    save_tf_graph           = False,    # Include full TensorFlow computation graph in the tfevents file?\n    save_weight_histograms  = False,    # Include weight histograms in the tfevents file?\n    resume_run_id           = \u0026amp;quot;latest\u0026amp;quot;, #None,     # Run ID or network pkl to resume training from, None = start from scratch.\n    resume_snapshot         = \u0026amp;#39;./results/00001-sgan-custom_dataset-1gpu/network-snapshot-.pkl\u0026amp;#39;, #None,     # Snapshot index to resume training from, None = autodetect.\n    resume_kimg             = 1040.9,      # Assumed training progress at the ...&quot;",
    td_title: "&quot;【StyleGAN入門】自前マシンでアニメの独自学習♬ &quot;"
  })</script><script async="" src="https://cdn.bigmining.com/private/js/qiita_bigmining.js"></script><div style="display:none"><div class="TagList__label"><span></span><span>Python</span></div><div class="TagList__label"><span></span><span>機械学習</span></div><div class="TagList__label"><span></span><span>DeepLearning</span></div><div class="TagList__label"><span></span><span>動画</span></div><div class="TagList__label"><span></span><span>stylegan</span></div></div><img style="display:block;margin:0;padding:0;border:0;outline:0;width:0;height:0;line-height:0;" alt="" src="https://relay-dsp.ad-m.asia/dmp/sync/bizmatrix?pid=c3ed207b574cf11376&amp;d=x18o8hduaj&amp;uid=595703" /><script type="application/json" id="js-react-on-rails-context">{"railsEnv":"production","inMailer":false,"i18nLocale":"ja","i18nDefaultLocale":"en","href":"https://qiita.com/MuAuan/items/aec7feabaa2f738ea82c","location":"/MuAuan/items/aec7feabaa2f738ea82c","scheme":"https","host":"qiita.com","port":null,"pathname":"/MuAuan/items/aec7feabaa2f738ea82c","search":null,"httpAcceptLanguage":"ja,en-US;q=0.9,en;q=0.8","actionPath":"public/items#show","settings":{"analyticsTrackingId":"UA-24675221-12","mixpanelToken":"17d24b448ca579c365d2d1057f3a1791","assetsMap":{},"csrfToken":"vpPAR96MwnkdsnRS8v81PCz1t/U79/23rDZ5MDoKDO1n5btQYIk34DAQCvWhlz0dXM7ZEj8ZfIROA/OgaFL2gg==","locale":"ja"},"currentUser":{"isStaff":false,"isJobseeker":false,"name":"","originalId":595703,"profileImageUrl":"https://qiita-user-profile-images.imgix.net/https%3A%2F%2Flh6.googleusercontent.com%2F-3EHSHCCSCmg%2FAAAAAAAAAAI%2FAAAAAAAAAAA%2FAKF05nAVp3wix0GPOS2jqZ7xSDRYCDF6Mw%2Fs50%2Fphoto.jpg?ixlib=rb-1.2.2\u0026auto=compress%2Cformat\u0026lossless=0\u0026w=48\u0026s=be4e5cacc9e0864ac1ef96d37914a7b9","urlName":"syoborian","remainingPublicImageUploadableSizeInCurrentMonth":104857600,"monthlyPublicImageUploadableSizeLimit":104857600},"isLoggedIn":true,"recaptchaSiteKey":"6LfNkiQTAAAAAM3UGnSquBy2akTITGNMO_QDxMw6","serverSide":false}</script>
<div id="PersonalArticle-react-component-b6c0f11d-cc2d-4c61-9a57-30cd0d392729"><div class="p-items logged-in"><div class="p-items_wrapper"><div class="p-items_container"><div class="p-items_main"><div class="p-items_article"><div class="it-Header"><div class="u-flex-center-between mb-3"><div class="it-Header_info"><div class="it-Header_author"><a href="/MuAuan"><img src="https://qiita-user-profile-images.imgix.net/https%3A%2F%2Favatars2.githubusercontent.com%2Fu%2F35788955%3Fv%3D4?ixlib=rb-1.2.2&amp;auto=compress%2Cformat&amp;lossless=0&amp;w=48&amp;s=c7c7473101694938ee68dd578e6ab883" alt="MuAuan" class="it-Header_authorImage"/></a><a href="/MuAuan" class="it-Header_authorName">@<!-- -->MuAuan</a></div><div class="it-Header_time"><span title="2020年01月23日に投稿" class=""><meta content="2020-01-23T14:28:45Z"/><time dateTime="2020-01-23T22:36:56Z">2020年01月23日に更新</time></span></div><div class="it-Header_meta"><div class="it-Header_manipulate"><div class="it-Header_dropdown"><span class="it-Header_dropdownToggle" tabindex="0"><span class="fa fa-ellipsis-h fa-lg"></span></span><div class="st-Dropdown right"><div><div class="it-Header_dropdown-title">記事の改善</div><a class="st-Dropdown_item" href="/drafts/aec7feabaa2f738ea82c/edit"><span class="fa fa-fw fa-code-fork pr-1"></span>編集リクエストを送る</a><div class="st-Dropdown_separator"></div></div><div class="it-Header_dropdown-title">記事の情報</div><a class="st-Dropdown_item" href="/MuAuan/items/aec7feabaa2f738ea82c/revisions"><span class="fa fa-fw fa-history pr-1"></span>編集履歴</a><a class="st-Dropdown_item" href="/MuAuan/items/aec7feabaa2f738ea82c/patches"><span class="fa fa-fw fa-inbox pr-1"></span>編集リクエスト一覧</a><a class="st-Dropdown_item" href="/MuAuan/items/aec7feabaa2f738ea82c/likers"><span style="margin-right:4px"><svg size="12" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 392.81 429" color="#777" class="VerticalLgtmIcon__Lgtm-sc-19v9h1n-0 pFoNU"><path d="M14.19 5.4h53.86v149.45h90.05v44.87H14.19zM288.4 93.77h100.79q1.29 25-5.66 45.39a96.79 96.79 0 01-20.33 34.89 92 92 0 01-32.13 22.45 104 104 0 01-40.95 7.93 109.71 109.71 0 01-76-29.92 104.05 104.05 0 01-23-32.56 95.46 95.46 0 01-8.47-39.88 94.78 94.78 0 018.47-39.87 104.38 104.38 0 0123-32.42A107.71 107.71 0 01248.23 8a110.79 110.79 0 01118.48 22.49l-35.07 35.08a51.25 51.25 0 00-17.75-15 52.83 52.83 0 00-44.67-1.23 52.92 52.92 0 00-17 12 57.07 57.07 0 00-11.45 18.11 60 60 0 00-4.23 22.77 60 60 0 004.23 22.68 56.57 56.57 0 0011.45 18.19 52.62 52.62 0 0017 12 50.5 50.5 0 0020.9 4.36q20.19 0 31.07-7.51a35.75 35.75 0 0014.46-20.55h-47.39zM51.29 279.55H0v-44.86h156v44.86h-51.13V429H51.29zM283.36 381.71l-41.72-62V429h-53.86V234.69h47.47L290 312l55.9-77.29h46.9V429h-53.86V320.27l-42.43 61.44z"></path></svg></span>LGTMしたユーザ一覧</a><a class="st-Dropdown_item" href="/MuAuan/items/aec7feabaa2f738ea82c.md"><span class="fa fa-fw fa-file-text-o pr-1"></span>Markdown で本文を見る</a><div class="st-Dropdown_separator"></div><div class="st-Dropdown_item"><span class="fa fa-fw fa-volume-up pr-1"></span>記事を購読する</div><div class="st-Dropdown_item"><span class="fa fa-fw fa-flag pr-1"></span>問題がある記事を報告する</div></div></div></div><div class="st-Modal"><div class="st-Modal_backdrop"></div><div class="st-Modal_body"><form><div class="st-Form"><span class="st-Form_label">この記事にどのような問題がありますか？</span></div><div class="st-Form"><label><input type="radio" name="reason" value="illegal" required=""/>法律違反（著作権侵害、プライバシー侵害、名誉棄損等）</label></div><div class="st-Form"><label><input type="radio" name="reason" value="inappropriate_content" required=""/>社会的に不適切（公序良俗に反する）</label></div><div class="st-Form"><label><input type="radio" name="reason" value="advertising" required=""/>宣伝行為</label></div><div class="st-Form"><label><input type="radio" name="reason" value="spam" required=""/>スパムの疑い</label></div><div class="st-Form"><label><input type="radio" name="reason" value="guideline_violation" required=""/>上記以外だが、Qiitaのコミュニティにふさわしくない（ガイドライン違反）</label></div><div class="st-Form st-Form-right"><input type="submit" class="st-Form_submit" value="送信"/></div></form></div></div></div></div></div><h1 class="it-Header_title">【StyleGAN入門】自前マシンでアニメの独自学習♬ </h1><div class="it-Tags"><a href="/tags/python" class="it-Tags_item"><span>Python</span></a><a href="/tags/%e6%a9%9f%e6%a2%b0%e5%ad%a6%e7%bf%92" class="it-Tags_item"><span>機械学習</span></a><a href="/tags/deeplearning" class="it-Tags_item"><span>DeepLearning</span></a><a href="/tags/%e5%8b%95%e7%94%bb" class="it-Tags_item"><span>動画</span></a><a href="/tags/stylegan" class="it-Tags_item"><span>stylegan</span></a></div></div><section class="it-MdContent"><div id="personal-public-article-body"><div><p>今回はStyleGANを使ってアニメ顔の学習に挑戦してみました。<br>
学習に関する参考はほとんどなく以下のものがありました。結構学習に時間もかかるし、情報も不十分でしたが、一応自前マシンで学習でき、かつ学習途中からの再学習もできたので、記事にまとめておきます。<br>
【参考】<br>
①<a href="https://evigio.com/post/how-to-use-custom-datasets-with-stylegan-tensorFlow-implementation" rel="nofollow noopener" target="_blank">How To Use Custom Datasets With StyleGAN - TensorFlow Implementation</a><br>
②<a href="http://blog.livedoor.jp/tak_tak0/archives/52409271.html" rel="nofollow noopener" target="_blank">styleganで独自モデルの学習方法</a><br>
③<a href="https://gist.github.com/eukaryote31/e7406e49f62f23e4004afa9788cafaa0" rel="nofollow noopener" target="_blank">StyleGAN log</a><br>
④<a href="https://www.gwern.net/Faces" rel="nofollow noopener" target="_blank">Making Anime Faces With StyleGAN</a></p>

<h3>
<span id="やったこと" class="fragment"></span><a href="#%E3%82%84%E3%81%A3%E3%81%9F%E3%81%93%E3%81%A8"><i class="fa fa-link"></i></a>やったこと</h3>

<p>・アニメ顔データの準備<br>
・とにかく学習する<br>
・潜在空間でのミキシングをやってみる<br>
・再学習するには</p>

<h3>
<span id="アニメ顔データの準備" class="fragment"></span><a href="#%E3%82%A2%E3%83%8B%E3%83%A1%E9%A1%94%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E6%BA%96%E5%82%99"><i class="fa fa-link"></i></a>・アニメ顔データの準備</h3>

<p>アニメ顔は<a href="https://qiita.com/MuAuan/items/85db7176574bdf979061" id="reference-c4df0e17f3243891c3e1">以前DCGANで利用したサイト</a>から、ダウンロードして準備しました。<br>
今回の利用のポイントは少なくとも画像サイズを合わせて、かつファイル名を変更して読み取りやすく、1.pngのように変更すること。<br>
StyleGANの学習という意味では目鼻顔などのアラインメントを合わせたいが、次回にパスした。<br>
ということで、上記のデータ整理は以下のコードで実施した。<br>
ちなみにサイズ（128,128）を1000個用意した。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="n">files</span> <span class="o">=</span> <span class="n">glob</span><span class="p">.</span><span class="n">glob</span><span class="p">(</span><span class="s">"./anime/**/*.png"</span><span class="p">,</span> <span class="n">recursive</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">files</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">sk</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="n">resize</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
    <span class="n">img</span><span class="p">.</span><span class="n">save</span><span class="p">(</span> <span class="s">"img/{}.png"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">sk</span><span class="p">))</span>
    <span class="n">sk</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div></div>

<h3>
<span id="とにかく学習する" class="fragment"></span><a href="#%E3%81%A8%E3%81%AB%E3%81%8B%E3%81%8F%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B"><i class="fa fa-link"></i></a>・とにかく学習する</h3>

<p>学習コードは、参考①のビデオと参考②を眺めつつ、以下のようにした。</p>

<p>kimgは学習image数であり、単位が千imgを意味している。<br>
総学習イメージ数が3400kimgを意味する<br>
次の行は学習開始解像度＝４<br>
そして、custom_datasetがtf_recordsに変換した画像のDir(＝datasets/custom_dataset)である。さらに、とりあえずの学習ということで、解像度=64で学習してみたが問題なく学習できた。<br>
また、StyleGANはpganであり、サイズ毎学習であるが、サイズ毎のminibatchサイズも以下のとおり、小さめに設定した。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre><span class="n">train</span><span class="p">.</span><span class="n">total_kimg</span> <span class="o">=</span> <span class="mi">3400</span><span class="err">、</span>
<span class="n">sched</span><span class="p">.</span><span class="n">lod_initial_resolution</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">desc</span> <span class="o">+=</span> <span class="s">'-custom_dataset'</span><span class="p">;</span>     <span class="n">dataset</span> <span class="o">=</span> <span class="n">EasyDict</span><span class="p">(</span><span class="n">tfrecord_dir</span><span class="o">=</span><span class="s">'custom_dataset'</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mi">64</span><span class="p">);</span>                 <span class="n">train</span><span class="p">.</span><span class="n">mirror_augment</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">desc</span> <span class="o">+=</span> <span class="s">'-1gpu'</span><span class="p">;</span> <span class="n">submit_config</span><span class="p">.</span><span class="n">num_gpus</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">sched</span><span class="p">.</span><span class="n">minibatch_base</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span> <span class="n">sched</span><span class="p">.</span><span class="n">minibatch_dict</span> <span class="o">=</span> <span class="p">{</span><span class="mi">4</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">8</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">16</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">128</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">256</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">512</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>
</pre></div></div>

<p>つまり、以下の必要最小限で動く。</p>

<div class="code-frame" data-lang="python">
<div class="code-lang"><span class="bold">train.py</span></div>
<div class="highlight"><pre><span class="c1"># Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.
#
# This work is licensed under the Creative Commons Attribution-NonCommercial
# 4.0 International License. To view a copy of this license, visit
# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to
# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.
</span>
<span class="s">"""Main entry point for training StyleGAN and ProGAN networks."""</span>

<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">dnnlib</span>
<span class="kn">from</span> <span class="nn">dnnlib</span> <span class="kn">import</span> <span class="n">EasyDict</span>

<span class="kn">import</span> <span class="nn">config</span>
<span class="kn">from</span> <span class="nn">metrics</span> <span class="kn">import</span> <span class="n">metric_base</span>

<span class="c1">#----------------------------------------------------------------------------
# Official training configs for StyleGAN, targeted mainly for FFHQ.
</span>
<span class="k">if</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">desc</span>          <span class="o">=</span> <span class="s">'sgan'</span>                                                                 <span class="c1"># Description string included in result subdir name.
</span>    <span class="n">train</span>         <span class="o">=</span> <span class="n">EasyDict</span><span class="p">(</span><span class="n">run_func_name</span><span class="o">=</span><span class="s">'training.training_loop.training_loop'</span><span class="p">)</span>         <span class="c1"># Options for training loop.
</span>    <span class="n">G</span>             <span class="o">=</span> <span class="n">EasyDict</span><span class="p">(</span><span class="n">func_name</span><span class="o">=</span><span class="s">'training.networks_stylegan.G_style'</span><span class="p">)</span>               <span class="c1"># Options for generator network.
</span>    <span class="n">D</span>             <span class="o">=</span> <span class="n">EasyDict</span><span class="p">(</span><span class="n">func_name</span><span class="o">=</span><span class="s">'training.networks_stylegan.D_basic'</span><span class="p">)</span>               <span class="c1"># Options for discriminator network.
</span>    <span class="n">G_opt</span>         <span class="o">=</span> <span class="n">EasyDict</span><span class="p">(</span><span class="n">beta1</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>                          <span class="c1"># Options for generator optimizer.
</span>    <span class="n">D_opt</span>         <span class="o">=</span> <span class="n">EasyDict</span><span class="p">(</span><span class="n">beta1</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>                          <span class="c1"># Options for discriminator optimizer.
</span>    <span class="n">G_loss</span>        <span class="o">=</span> <span class="n">EasyDict</span><span class="p">(</span><span class="n">func_name</span><span class="o">=</span><span class="s">'training.loss.G_logistic_nonsaturating'</span><span class="p">)</span>           <span class="c1"># Options for generator loss.
</span>    <span class="n">D_loss</span>        <span class="o">=</span> <span class="n">EasyDict</span><span class="p">(</span><span class="n">func_name</span><span class="o">=</span><span class="s">'training.loss.D_logistic_simplegp'</span><span class="p">,</span> <span class="n">r1_gamma</span><span class="o">=</span><span class="mf">10.0</span><span class="p">)</span> <span class="c1"># Options for discriminator loss.
</span>    <span class="n">dataset</span>       <span class="o">=</span> <span class="n">EasyDict</span><span class="p">()</span>                                                             <span class="c1"># Options for load_dataset().
</span>    <span class="n">sched</span>         <span class="o">=</span> <span class="n">EasyDict</span><span class="p">()</span>                                                             <span class="c1"># Options for TrainingSchedule.
</span>    <span class="n">grid</span>          <span class="o">=</span> <span class="n">EasyDict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="s">'4k'</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="s">'random'</span><span class="p">)</span>  <span class="c1">#4k                              # Options for setup_snapshot_image_grid().
</span>    <span class="n">metrics</span>       <span class="o">=</span> <span class="p">[</span><span class="n">metric_base</span><span class="p">.</span><span class="n">fid50k</span><span class="p">]</span>                                                   <span class="c1"># Options for MetricGroup.
</span>    <span class="n">submit_config</span> <span class="o">=</span> <span class="n">dnnlib</span><span class="p">.</span><span class="n">SubmitConfig</span><span class="p">()</span>                                                  <span class="c1"># Options for dnnlib.submit_run().
</span>    <span class="n">tf_config</span>     <span class="o">=</span> <span class="p">{</span><span class="s">'rnd.np_random_seed'</span><span class="p">:</span> <span class="mi">1000</span><span class="p">}</span>                                           <span class="c1"># Options for tflib.init_tf().
</span>
    <span class="c1"># Dataset.
</span>    <span class="n">desc</span> <span class="o">+=</span> <span class="s">'-custom_dataset'</span><span class="p">;</span>     <span class="n">dataset</span> <span class="o">=</span> <span class="n">EasyDict</span><span class="p">(</span><span class="n">tfrecord_dir</span><span class="o">=</span><span class="s">'custom_dataset'</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mi">64</span><span class="p">);</span>                 <span class="n">train</span><span class="p">.</span><span class="n">mirror_augment</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="c1"># Number of GPUs.
</span>    <span class="n">desc</span> <span class="o">+=</span> <span class="s">'-1gpu'</span><span class="p">;</span> <span class="n">submit_config</span><span class="p">.</span><span class="n">num_gpus</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">sched</span><span class="p">.</span><span class="n">minibatch_base</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span> <span class="n">sched</span><span class="p">.</span><span class="n">minibatch_dict</span> <span class="o">=</span> <span class="p">{</span><span class="mi">4</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">8</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">16</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">128</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">256</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">512</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>

    <span class="c1"># Default options.
</span>    <span class="n">train</span><span class="p">.</span><span class="n">total_kimg</span> <span class="o">=</span> <span class="mi">3400</span>
    <span class="n">sched</span><span class="p">.</span><span class="n">lod_initial_resolution</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">sched</span><span class="p">.</span><span class="n">G_lrate_dict</span> <span class="o">=</span> <span class="p">{</span><span class="mi">128</span><span class="p">:</span> <span class="mf">0.0015</span><span class="p">,</span> <span class="mi">256</span><span class="p">:</span> <span class="mf">0.002</span><span class="p">,</span> <span class="mi">512</span><span class="p">:</span> <span class="mf">0.003</span><span class="p">,</span> <span class="mi">1024</span><span class="p">:</span> <span class="mf">0.003</span><span class="p">}</span>
    <span class="n">sched</span><span class="p">.</span><span class="n">D_lrate_dict</span> <span class="o">=</span> <span class="n">EasyDict</span><span class="p">(</span><span class="n">sched</span><span class="p">.</span><span class="n">G_lrate_dict</span><span class="p">)</span>

<span class="c1">#----------------------------------------------------------------------------
# Main entry point for training.
# Calls the function indicated by 'train' using the selected options.
</span>
<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">EasyDict</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>
    <span class="n">kwargs</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">G_args</span><span class="o">=</span><span class="n">G</span><span class="p">,</span> <span class="n">D_args</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">G_opt_args</span><span class="o">=</span><span class="n">G_opt</span><span class="p">,</span> <span class="n">D_opt_args</span><span class="o">=</span><span class="n">D_opt</span><span class="p">,</span> <span class="n">G_loss_args</span><span class="o">=</span><span class="n">G_loss</span><span class="p">,</span> <span class="n">D_loss_args</span><span class="o">=</span><span class="n">D_loss</span><span class="p">)</span>
    <span class="n">kwargs</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">dataset_args</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">sched_args</span><span class="o">=</span><span class="n">sched</span><span class="p">,</span> <span class="n">grid_args</span><span class="o">=</span><span class="n">grid</span><span class="p">,</span> <span class="n">metric_arg_list</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span> <span class="n">tf_config</span><span class="o">=</span><span class="n">tf_config</span><span class="p">)</span>
    <span class="n">kwargs</span><span class="p">.</span><span class="n">submit_config</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">submit_config</span><span class="p">)</span>
    <span class="n">kwargs</span><span class="p">.</span><span class="n">submit_config</span><span class="p">.</span><span class="n">run_dir_root</span> <span class="o">=</span> <span class="n">dnnlib</span><span class="p">.</span><span class="n">submission</span><span class="p">.</span><span class="n">submit</span><span class="p">.</span><span class="n">get_template_from_path</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">result_dir</span><span class="p">)</span>
    <span class="n">kwargs</span><span class="p">.</span><span class="n">submit_config</span><span class="p">.</span><span class="n">run_dir_ignore</span> <span class="o">+=</span> <span class="n">config</span><span class="p">.</span><span class="n">run_dir_ignore</span>
    <span class="n">kwargs</span><span class="p">.</span><span class="n">submit_config</span><span class="p">.</span><span class="n">run_desc</span> <span class="o">=</span> <span class="n">desc</span>
    <span class="n">dnnlib</span><span class="p">.</span><span class="n">submit_run</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="c1">#----------------------------------------------------------------------------
</span>
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>

<span class="c1">#----------------------------------------------------------------------------
</span></pre></div>
</div>

<p>なお、データのtfrecordsへの変換は参考①から以下のように実施した。<br>
※ここで元画像は./animeに入れておく、そして画像サイズ毎の変換ファイルはcustom_datasetに６個のサイズの異なるファイルが格納された。</p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre>python dataset_tool.py create_from_images datasets/custom_dataset ./anime
</pre></div></div>

<p>上記でとりあえず、学習できると思う。</p>

<h3>
<span id="潜在空間でのミキシングをやってみる" class="fragment"></span><a href="#%E6%BD%9C%E5%9C%A8%E7%A9%BA%E9%96%93%E3%81%A7%E3%81%AE%E3%83%9F%E3%82%AD%E3%82%B7%E3%83%B3%E3%82%B0%E3%82%92%E3%82%84%E3%81%A3%E3%81%A6%E3%81%BF%E3%82%8B"><i class="fa fa-link"></i></a>・潜在空間でのミキシングをやってみる</h3>

<p>1060マシンで上記のコードを10h程度で以下の絵が得られる。<br>
決して綺麗とは言えないが、とにかく最弱マシンでも学習できた。<br>
<a href="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F5a66106f-1906-e5e3-718d-402b320f922b.jpeg?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=d785fce7148ed01bf434160828677a1c" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F5a66106f-1906-e5e3-718d-402b320f922b.jpeg?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=d785fce7148ed01bf434160828677a1c" alt="64x64_3400.jpg" data-canonical-src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/233744/5a66106f-1906-e5e3-718d-402b320f922b.jpeg" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F5a66106f-1906-e5e3-718d-402b320f922b.jpeg?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=3d3c709f538fc75a5bec571e5086bd7d 1x" loading="lazy"></a><br>
さらに、潜在空間での17，18のミキシングをやってみると以下の絵が得られた。<br>
<a href="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F99bb4f44-d8d1-6ca7-aa65-08f087cf9601.gif?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=a6f3e8224cbacc15f006258effd8fbb1" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F99bb4f44-d8d1-6ca7-aa65-08f087cf9601.gif?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=a6f3e8224cbacc15f006258effd8fbb1" alt="example17_18.gif" data-canonical-src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/233744/99bb4f44-d8d1-6ca7-aa65-08f087cf9601.gif" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F99bb4f44-d8d1-6ca7-aa65-08f087cf9601.gif?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=5a9695722864ecaa3da7cf176a03babb 1x" loading="lazy"></a><br>
1080マシンで解像度128x128サイズで1d8h程度回して、kimg=4705の場合以下のように画像がしっかりしてきた。<br>
※これでもpid50K=168程度で精度はまだまだだが、。。。<a href="https://qiita.com/MuAuan/items/85db7176574bdf979061">以前のDCGANの画像</a>と比べてこちらの方が綺麗に見える<br>
<a href="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F6ce25027-b303-4de4-83fc-3909c57bf178.jpeg?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=a526b17da1157b8e799a6c003a7d8f6e" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F6ce25027-b303-4de4-83fc-3909c57bf178.jpeg?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=a526b17da1157b8e799a6c003a7d8f6e" alt="128x128_4705_25.jpg" data-canonical-src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/233744/6ce25027-b303-4de4-83fc-3909c57bf178.jpeg" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F6ce25027-b303-4de4-83fc-3909c57bf178.jpeg?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=e54051ebfe78c5d3c2d53e4f103c96e7 1x" loading="lazy"></a><br>
さらに、潜在空間での11，82のミキシングをやってみると以下の絵が得られた。<br>
<a href="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F93df08de-f7a7-25d7-90ca-91a248fd84f2.gif?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=86bf6c81b791eba87c980cda003886da" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F93df08de-f7a7-25d7-90ca-91a248fd84f2.gif?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=86bf6c81b791eba87c980cda003886da" alt="example_128_4705_100_82x11.gif" data-canonical-src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/233744/93df08de-f7a7-25d7-90ca-91a248fd84f2.gif" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F93df08de-f7a7-25d7-90ca-91a248fd84f2.gif?ixlib=rb-1.2.2&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=0954e31f6c5e1e16d85c815c15ad2244 1x" loading="lazy"></a></p>

<h3>
<span id="再学習するには" class="fragment"></span><a href="#%E5%86%8D%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B%E3%81%AB%E3%81%AF"><i class="fa fa-link"></i></a>・再学習するには</h3>

<p>最後に禁断（誰も公開していないようなので）の途中中断した場合の継続学習の仕方が出来たので、まとめておく。<br>
※この方法はちょっとだけ参考②と参考④に記載がある<br>
以下のコードで実施する。<br>
すなわち、training_loop.pyの以下の部分を修正する。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>    <span class="n">resume_run_id</span>           <span class="o">=</span> <span class="s">"latest"</span><span class="p">,</span> <span class="c1">#None,     # Run ID or network pkl to resume training from, None = start from scratch.
</span>    <span class="n">resume_snapshot</span>         <span class="o">=</span> <span class="s">'./results/00001-sgan-custom_dataset-1gpu/network-snapshot-.pkl'</span><span class="p">,</span> <span class="c1">#None,     # Snapshot index to resume training from, None = autodetect.
</span></pre></div></div>

<p>また、network_snapshot_ticks  = 1,       # How often to export network snapshots? として、毎回出力にしている。</p>

<div class="code-frame" data-lang="python">
<div class="code-lang"><span class="bold">training_loop.py</span></div>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">training_loop</span><span class="p">(</span>
    <span class="n">submit_config</span><span class="p">,</span>
    <span class="n">G_args</span>                  <span class="o">=</span> <span class="p">{},</span>       <span class="c1"># Options for generator network.
</span>    <span class="n">D_args</span>                  <span class="o">=</span> <span class="p">{},</span>       <span class="c1"># Options for discriminator network.
</span>    <span class="n">G_opt_args</span>              <span class="o">=</span> <span class="p">{},</span>       <span class="c1"># Options for generator optimizer.
</span>    <span class="n">D_opt_args</span>              <span class="o">=</span> <span class="p">{},</span>       <span class="c1"># Options for discriminator optimizer.
</span>    <span class="n">G_loss_args</span>             <span class="o">=</span> <span class="p">{},</span>       <span class="c1"># Options for generator loss.
</span>    <span class="n">D_loss_args</span>             <span class="o">=</span> <span class="p">{},</span>       <span class="c1"># Options for discriminator loss.
</span>    <span class="n">dataset_args</span>            <span class="o">=</span> <span class="p">{},</span>       <span class="c1"># Options for dataset.load_dataset().
</span>    <span class="n">sched_args</span>              <span class="o">=</span> <span class="p">{},</span>       <span class="c1"># Options for train.TrainingSchedule.
</span>    <span class="n">grid_args</span>               <span class="o">=</span> <span class="p">{},</span>       <span class="c1"># Options for train.setup_snapshot_image_grid().
</span>    <span class="n">metric_arg_list</span>         <span class="o">=</span> <span class="p">[],</span>       <span class="c1"># Options for MetricGroup.
</span>    <span class="n">tf_config</span>               <span class="o">=</span> <span class="p">{},</span>       <span class="c1"># Options for tflib.init_tf().
</span>    <span class="n">G_smoothing_kimg</span>        <span class="o">=</span> <span class="mf">10.0</span><span class="p">,</span>     <span class="c1"># Half-life of the running average of generator weights.
</span>    <span class="n">D_repeats</span>               <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># How many times the discriminator is trained per G iteration.
</span>    <span class="n">minibatch_repeats</span>       <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>        <span class="c1"># Number of minibatches to run before adjusting training parameters.
</span>    <span class="n">reset_opt_for_new_lod</span>   <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>     <span class="c1"># Reset optimizer internal state (e.g. Adam moments) when new layers are introduced?
</span>    <span class="n">total_kimg</span>              <span class="o">=</span> <span class="mi">15000</span><span class="p">,</span>    <span class="c1"># Total length of the training, measured in thousands of real images.
</span>    <span class="n">mirror_augment</span>          <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>    <span class="c1"># Enable mirror augment?
</span>    <span class="n">drange_net</span>              <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>   <span class="c1"># Dynamic range used when feeding image data to the networks.
</span>    <span class="n">image_snapshot_ticks</span>    <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># How often to export image snapshots?
</span>    <span class="n">network_snapshot_ticks</span>  <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>       <span class="c1"># How often to export network snapshots? default=10
</span>    <span class="n">save_tf_graph</span>           <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>    <span class="c1"># Include full TensorFlow computation graph in the tfevents file?
</span>    <span class="n">save_weight_histograms</span>  <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>    <span class="c1"># Include weight histograms in the tfevents file?
</span>    <span class="n">resume_run_id</span>           <span class="o">=</span> <span class="s">"latest"</span><span class="p">,</span> <span class="c1">#None,     # Run ID or network pkl to resume training from, None = start from scratch.
</span>    <span class="n">resume_snapshot</span>         <span class="o">=</span> <span class="s">'./results/00001-sgan-custom_dataset-1gpu/network-snapshot-.pkl'</span><span class="p">,</span> <span class="c1">#None,     # Snapshot index to resume training from, None = autodetect.
</span>    <span class="n">resume_kimg</span>             <span class="o">=</span> <span class="mf">1040.9</span><span class="p">,</span>      <span class="c1"># Assumed training progress at the beginning. Affects reporting and training schedule.
</span>    <span class="n">resume_time</span>             <span class="o">=</span> <span class="mf">5599.0</span><span class="p">):</span>     <span class="c1"># Assumed wallclock time at the beginning. Affects reporting.
</span></pre></div>
</div>

<p>resume_time             = 5599.0<br>
は秒単位で入力する。<br>
このままだとメモリーがたまらないので、さらに上書き保存するためにもう一か所、下のコードのように変更した。</p>

<div class="code-frame" data-lang="python">
<div class="code-lang"><span class="bold">train_loops.py</span></div>
<div class="highlight"><pre><span class="k">if</span> <span class="n">cur_tick</span> <span class="o">%</span> <span class="n">network_snapshot_ticks</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">cur_tick</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1">#pkl = os.path.join(submit_config.run_dir, 'network-snapshot-%06d.pkl' % (cur_nimg // 1000))
</span>                <span class="n">pkl</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">submit_config</span><span class="p">.</span><span class="n">run_dir</span><span class="p">,</span> <span class="s">'network-snapshot-.pkl'</span><span class="p">)</span>
                <span class="n">misc</span><span class="p">.</span><span class="n">save_pkl</span><span class="p">((</span><span class="n">G</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">Gs</span><span class="p">),</span> <span class="n">pkl</span><span class="p">)</span>
                <span class="n">metrics</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">pkl</span><span class="p">,</span> <span class="n">run_dir</span><span class="o">=</span><span class="n">submit_config</span><span class="p">.</span><span class="n">run_dir</span><span class="p">,</span> <span class="n">num_gpus</span><span class="o">=</span><span class="n">submit_config</span><span class="p">.</span><span class="n">num_gpus</span><span class="p">,</span> <span class="n">tf_config</span><span class="o">=</span><span class="n">tf_config</span><span class="p">)</span>
</pre></div>
</div>

<p>計算が1080マシンでもかなりかかるので、これを利用した結果については後日公開したいと思う。</p>

<h3>
<span id="まとめ" class="fragment"></span><a href="#%E3%81%BE%E3%81%A8%E3%82%81"><i class="fa fa-link"></i></a>まとめ</h3>

<p>・自前マシンでStyleGANの学習ができるようになった<br>
・学習データで今まで報告したようなミキシングが出来た<br>
・途中で中断した場合の再開の仕方が分かった</p>

<p>・今回は1000データだが、100以下の少数データの結果も見ようと思う<br>
・さらに精度を追求してスタイルなどもやってみようと思う</p>

<h1>
<span id="おまけ" class="fragment"></span><a href="#%E3%81%8A%E3%81%BE%E3%81%91"><i class="fa fa-link"></i></a>おまけ</h1>

<div class="code-frame" data-lang="text"><div class="highlight"><pre>dnnlib: Running training.training_loop.training_loop() on localhost...
Streaming data using training.dataset.TFRecordDataset...
Dataset shape = [3, 64, 64]
Dynamic range = [0, 255]
Label size    = 0
Constructing networks...

G                           Params    OutputShape       WeightShape     
---                         ---       ---               ---             
latents_in                  -         (?, 512)          -               
labels_in                   -         (?, 0)            -               
lod                         -         ()                -               
dlatent_avg                 -         (512,)            -               
G_mapping/latents_in        -         (?, 512)          -               
G_mapping/labels_in         -         (?, 0)            -               
G_mapping/PixelNorm         -         (?, 512)          -               
G_mapping/Dense0            262656    (?, 512)          (512, 512)      
G_mapping/Dense1            262656    (?, 512)          (512, 512)      
G_mapping/Dense2            262656    (?, 512)          (512, 512)      
G_mapping/Dense3            262656    (?, 512)          (512, 512)      
G_mapping/Dense4            262656    (?, 512)          (512, 512)      
G_mapping/Dense5            262656    (?, 512)          (512, 512)      
G_mapping/Dense6            262656    (?, 512)          (512, 512)      
G_mapping/Dense7            262656    (?, 512)          (512, 512)      
G_mapping/Broadcast         -         (?, 10, 512)      -               
G_mapping/dlatents_out      -         (?, 10, 512)      -               
Truncation                  -         (?, 10, 512)      -               
G_synthesis/dlatents_in     -         (?, 10, 512)      -               
G_synthesis/4x4/Const       534528    (?, 512, 4, 4)    (512,)          
G_synthesis/4x4/Conv        2885632   (?, 512, 4, 4)    (3, 3, 512, 512)
G_synthesis/ToRGB_lod4      1539      (?, 3, 4, 4)      (1, 1, 512, 3)  
G_synthesis/8x8/Conv0_up    2885632   (?, 512, 8, 8)    (3, 3, 512, 512)
G_synthesis/8x8/Conv1       2885632   (?, 512, 8, 8)    (3, 3, 512, 512)
G_synthesis/ToRGB_lod3      1539      (?, 3, 8, 8)      (1, 1, 512, 3)  
G_synthesis/Upscale2D       -         (?, 3, 8, 8)      -               
G_synthesis/Grow_lod3       -         (?, 3, 8, 8)      -               
G_synthesis/16x16/Conv0_up  2885632   (?, 512, 16, 16)  (3, 3, 512, 512)
G_synthesis/16x16/Conv1     2885632   (?, 512, 16, 16)  (3, 3, 512, 512)
G_synthesis/ToRGB_lod2      1539      (?, 3, 16, 16)    (1, 1, 512, 3)  
G_synthesis/Upscale2D_1     -         (?, 3, 16, 16)    -               
G_synthesis/Grow_lod2       -         (?, 3, 16, 16)    -               
G_synthesis/32x32/Conv0_up  2885632   (?, 512, 32, 32)  (3, 3, 512, 512)
G_synthesis/32x32/Conv1     2885632   (?, 512, 32, 32)  (3, 3, 512, 512)
G_synthesis/ToRGB_lod1      1539      (?, 3, 32, 32)    (1, 1, 512, 3)  
G_synthesis/Upscale2D_2     -         (?, 3, 32, 32)    -               
G_synthesis/Grow_lod1       -         (?, 3, 32, 32)    -               
G_synthesis/64x64/Conv0_up  1442816   (?, 256, 64, 64)  (3, 3, 512, 256)
G_synthesis/64x64/Conv1     852992    (?, 256, 64, 64)  (3, 3, 256, 256)
G_synthesis/ToRGB_lod0      771       (?, 3, 64, 64)    (1, 1, 256, 3)  
G_synthesis/Upscale2D_3     -         (?, 3, 64, 64)    -               
G_synthesis/Grow_lod0       -         (?, 3, 64, 64)    -               
G_synthesis/images_out      -         (?, 3, 64, 64)    -               
G_synthesis/lod             -         ()                -               
G_synthesis/noise0          -         (1, 1, 4, 4)      -               
G_synthesis/noise1          -         (1, 1, 4, 4)      -               
G_synthesis/noise2          -         (1, 1, 8, 8)      -               
G_synthesis/noise3          -         (1, 1, 8, 8)      -               
G_synthesis/noise4          -         (1, 1, 16, 16)    -               
G_synthesis/noise5          -         (1, 1, 16, 16)    -               
G_synthesis/noise6          -         (1, 1, 32, 32)    -               
G_synthesis/noise7          -         (1, 1, 32, 32)    -               
G_synthesis/noise8          -         (1, 1, 64, 64)    -               
G_synthesis/noise9          -         (1, 1, 64, 64)    -               
images_out                  -         (?, 3, 64, 64)    -               
---                         ---       ---               ---             
Total                       25137935                                    


D                    Params    OutputShape       WeightShape     
---                  ---       ---               ---             
images_in            -         (?, 3, 64, 64)    -               
labels_in            -         (?, 0)            -               
lod                  -         ()                -               
FromRGB_lod0         1024      (?, 256, 64, 64)  (1, 1, 3, 256)  
64x64/Conv0          590080    (?, 256, 64, 64)  (3, 3, 256, 256)
64x64/Conv1_down     1180160   (?, 512, 32, 32)  (3, 3, 256, 512)
Downscale2D          -         (?, 3, 32, 32)    -               
FromRGB_lod1         2048      (?, 512, 32, 32)  (1, 1, 3, 512)  
Grow_lod0            -         (?, 512, 32, 32)  -               
32x32/Conv0          2359808   (?, 512, 32, 32)  (3, 3, 512, 512)
32x32/Conv1_down     2359808   (?, 512, 16, 16)  (3, 3, 512, 512)
Downscale2D_1        -         (?, 3, 16, 16)    -               
FromRGB_lod2         2048      (?, 512, 16, 16)  (1, 1, 3, 512)  
Grow_lod1            -         (?, 512, 16, 16)  -               
16x16/Conv0          2359808   (?, 512, 16, 16)  (3, 3, 512, 512)
16x16/Conv1_down     2359808   (?, 512, 8, 8)    (3, 3, 512, 512)
Downscale2D_2        -         (?, 3, 8, 8)      -               
FromRGB_lod3         2048      (?, 512, 8, 8)    (1, 1, 3, 512)  
Grow_lod2            -         (?, 512, 8, 8)    -               
8x8/Conv0            2359808   (?, 512, 8, 8)    (3, 3, 512, 512)
8x8/Conv1_down       2359808   (?, 512, 4, 4)    (3, 3, 512, 512)
Downscale2D_3        -         (?, 3, 4, 4)      -               
FromRGB_lod4         2048      (?, 512, 4, 4)    (1, 1, 3, 512)  
Grow_lod3            -         (?, 512, 4, 4)    -               
4x4/MinibatchStddev  -         (?, 513, 4, 4)    -               
4x4/Conv             2364416   (?, 512, 4, 4)    (3, 3, 513, 512)
4x4/Dense0           4194816   (?, 512)          (8192, 512)     
4x4/Dense1           513       (?, 1)            (512, 1)        
scores_out           -         (?, 1)            -               
---                  ---       ---               ---             
Total                22498049                                    

Building TensorFlow graph...

WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

Setting up snapshot image grid...
Setting up run dir...
Training...

tick 1     kimg 160.3    lod 4.00  minibatch 128  time 5m 35s       sec/tick 297.2   sec/kimg 1.85    maintenance 38.0   gpumem 1.7 
network-snapshot-000160        time 16m 22s      fid50k 454.0154  
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
tick 2     kimg 320.5    lod 4.00  minibatch 128  time 26m 00s      sec/tick 222.0   sec/kimg 1.39    maintenance 1002.8 gpumem 2.0 
tick 3     kimg 480.8    lod 4.00  minibatch 128  time 29m 43s      sec/tick 222.0   sec/kimg 1.38    maintenance 1.4    gpumem 2.0 
tick 4     kimg 620.8    lod 3.97  minibatch 64   time 33m 41s      sec/tick 236.2   sec/kimg 1.69    maintenance 1.2    gpumem 2.0 
tick 5     kimg 760.8    lod 3.73  minibatch 64   time 41m 24s      sec/tick 462.3   sec/kimg 3.30    maintenance 1.3    gpumem 2.0 
tick 6     kimg 900.9    lod 3.50  minibatch 64   time 49m 07s      sec/tick 461.2   sec/kimg 3.29    maintenance 1.3    gpumem 2.0 
tick 7     kimg 1040.9   lod 3.27  minibatch 64   time 56m 49s      sec/tick 461.2   sec/kimg 3.29    maintenance 1.3    gpumem 2.0 
tick 8     kimg 1180.9   lod 3.03  minibatch 64   time 1h 04m 31s   sec/tick 460.2   sec/kimg 3.29    maintenance 1.3    gpumem 2.0 
tick 9     kimg 1321.0   lod 3.00  minibatch 64   time 1h 12m 06s   sec/tick 453.5   sec/kimg 3.24    maintenance 1.3    gpumem 2.0 
tick 10    kimg 1461.0   lod 3.00  minibatch 64   time 1h 19m 40s   sec/tick 452.6   sec/kimg 3.23    maintenance 1.3    gpumem 2.0 
network-snapshot-001460        time 8m 33s       fid50k 378.7820  
tick 11    kimg 1601.0   lod 3.00  minibatch 64   time 1h 35m 49s   sec/tick 453.8   sec/kimg 3.24    maintenance 515.6  gpumem 2.0 
tick 12    kimg 1741.1   lod 3.00  minibatch 64   time 1h 43m 24s   sec/tick 453.8   sec/kimg 3.24    maintenance 1.3    gpumem 2.0 
tick 13    kimg 1861.1   lod 2.90  minibatch 32   time 1h 57m 38s   sec/tick 852.2   sec/kimg 7.10    maintenance 1.3    gpumem 2.0 
tick 14    kimg 1981.2   lod 2.70  minibatch 32   time 2h 18m 55s   sec/tick 1275.3  sec/kimg 10.62   maintenance 2.0    gpumem 2.0 
tick 15    kimg 2101.2   lod 2.50  minibatch 32   time 2h 40m 10s   sec/tick 1273.1  sec/kimg 10.60   maintenance 1.9    gpumem 2.0 
tick 16    kimg 2221.3   lod 2.30  minibatch 32   time 3h 01m 25s   sec/tick 1273.0  sec/kimg 10.60   maintenance 1.9    gpumem 2.0 
tick 17    kimg 2341.4   lod 2.10  minibatch 32   time 3h 22m 42s   sec/tick 1275.0  sec/kimg 10.62   maintenance 1.9    gpumem 2.0 
tick 18    kimg 2461.4   lod 2.00  minibatch 32   time 3h 43m 49s   sec/tick 1265.4  sec/kimg 10.54   maintenance 1.9    gpumem 2.0 
tick 19    kimg 2581.5   lod 2.00  minibatch 32   time 4h 04m 45s   sec/tick 1253.8  sec/kimg 10.44   maintenance 1.9    gpumem 2.0 
tick 20    kimg 2701.6   lod 2.00  minibatch 32   time 4h 25m 41s   sec/tick 1254.5  sec/kimg 10.45   maintenance 1.9    gpumem 2.0 
network-snapshot-002701        time 9m 08s       fid50k 338.4830  
tick 21    kimg 2821.6   lod 2.00  minibatch 32   time 4h 55m 47s   sec/tick 1255.4  sec/kimg 10.46   maintenance 551.1  gpumem 2.0 
tick 22    kimg 2941.7   lod 2.00  minibatch 32   time 5h 16m 44s   sec/tick 1254.7  sec/kimg 10.45   maintenance 1.8    gpumem 2.0 
tick 23    kimg 3041.7   lod 1.93  minibatch 16   time 5h 52m 23s   sec/tick 2136.8  sec/kimg 21.36   maintenance 1.8    gpumem 2.0 
tick 24    kimg 3141.8   lod 1.76  minibatch 16   time 6h 52m 21s   sec/tick 3593.7  sec/kimg 35.93   maintenance 4.5    gpumem 2.0 
tick 25    kimg 3241.8   lod 1.60  minibatch 16   time 7h 52m 23s   sec/tick 3597.7  sec/kimg 35.97   maintenance 4.5    gpumem 2.0 
tick 26    kimg 3341.8   lod 1.43  minibatch 16   time 8h 52m 34s   sec/tick 3606.5  sec/kimg 36.05   maintenance 4.6    gpumem 2.0 
tick 27    kimg 3400.0   lod 1.33  minibatch 16   time 9h 27m 29s   sec/tick 2090.0  sec/kimg 35.92   maintenance 4.6    gpumem 2.0 
network-snapshot-003400        time 11m 15s      fid50k 327.9088  
dnnlib: Finished training.training_loop.training_loop() in 9h 38m 52s.
</pre></div></div>

<div class="code-frame" data-lang="text"><div class="highlight"><pre>(keras-gpu) C:\Users\user\stylegan-master&gt;python train.py
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorflow\python\framework\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorflow\python\framework\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorflow\python\framework\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorflow\python\framework\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorflow\python\framework\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorflow\python\framework\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Creating the run dir: results\00004-sgan-custom_dataset-1gpu
Copying files to the run dir
dnnlib: Running training.training_loop.training_loop() on localhost...
Streaming data using training.dataset.TFRecordDataset...
WARNING:tensorflow:From C:\Users\user\stylegan-master\training\dataset.py:76: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.
Instructions for updating:
Use eager execution and:
`tf.data.TFRecordDataset(path)`
WARNING:tensorflow:From C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Dataset shape = [3, 128, 128]
Dynamic range = [0, 255]
Label size    = 0
Constructing networks...

G                             Params    OutputShape         WeightShape
---                           ---       ---                 ---
latents_in                    -         (?, 512)            -
labels_in                     -         (?, 0)              -
lod                           -         ()                  -
dlatent_avg                   -         (512,)              -
G_mapping/latents_in          -         (?, 512)            -
G_mapping/labels_in           -         (?, 0)              -
G_mapping/PixelNorm           -         (?, 512)            -
G_mapping/Dense0              262656    (?, 512)            (512, 512)
G_mapping/Dense1              262656    (?, 512)            (512, 512)
G_mapping/Dense2              262656    (?, 512)            (512, 512)
G_mapping/Dense3              262656    (?, 512)            (512, 512)
G_mapping/Dense4              262656    (?, 512)            (512, 512)
G_mapping/Dense5              262656    (?, 512)            (512, 512)
G_mapping/Dense6              262656    (?, 512)            (512, 512)
G_mapping/Dense7              262656    (?, 512)            (512, 512)
G_mapping/Broadcast           -         (?, 12, 512)        -
G_mapping/dlatents_out        -         (?, 12, 512)        -
Truncation                    -         (?, 12, 512)        -
G_synthesis/dlatents_in       -         (?, 12, 512)        -
G_synthesis/4x4/Const         534528    (?, 512, 4, 4)      (512,)
G_synthesis/4x4/Conv          2885632   (?, 512, 4, 4)      (3, 3, 512, 512)
G_synthesis/ToRGB_lod5        1539      (?, 3, 4, 4)        (1, 1, 512, 3)
G_synthesis/8x8/Conv0_up      2885632   (?, 512, 8, 8)      (3, 3, 512, 512)
G_synthesis/8x8/Conv1         2885632   (?, 512, 8, 8)      (3, 3, 512, 512)
G_synthesis/ToRGB_lod4        1539      (?, 3, 8, 8)        (1, 1, 512, 3)
G_synthesis/Upscale2D         -         (?, 3, 8, 8)        -
G_synthesis/Grow_lod4         -         (?, 3, 8, 8)        -
G_synthesis/16x16/Conv0_up    2885632   (?, 512, 16, 16)    (3, 3, 512, 512)
G_synthesis/16x16/Conv1       2885632   (?, 512, 16, 16)    (3, 3, 512, 512)
G_synthesis/ToRGB_lod3        1539      (?, 3, 16, 16)      (1, 1, 512, 3)
G_synthesis/Upscale2D_1       -         (?, 3, 16, 16)      -
G_synthesis/Grow_lod3         -         (?, 3, 16, 16)      -
G_synthesis/32x32/Conv0_up    2885632   (?, 512, 32, 32)    (3, 3, 512, 512)
G_synthesis/32x32/Conv1       2885632   (?, 512, 32, 32)    (3, 3, 512, 512)
G_synthesis/ToRGB_lod2        1539      (?, 3, 32, 32)      (1, 1, 512, 3)
G_synthesis/Upscale2D_2       -         (?, 3, 32, 32)      -
G_synthesis/Grow_lod2         -         (?, 3, 32, 32)      -
G_synthesis/64x64/Conv0_up    1442816   (?, 256, 64, 64)    (3, 3, 512, 256)
G_synthesis/64x64/Conv1       852992    (?, 256, 64, 64)    (3, 3, 256, 256)
G_synthesis/ToRGB_lod1        771       (?, 3, 64, 64)      (1, 1, 256, 3)
G_synthesis/Upscale2D_3       -         (?, 3, 64, 64)      -
G_synthesis/Grow_lod1         -         (?, 3, 64, 64)      -
G_synthesis/128x128/Conv0_up  426496    (?, 128, 128, 128)  (3, 3, 256, 128)
G_synthesis/128x128/Conv1     279040    (?, 128, 128, 128)  (3, 3, 128, 128)
G_synthesis/ToRGB_lod0        387       (?, 3, 128, 128)    (1, 1, 128, 3)
G_synthesis/Upscale2D_4       -         (?, 3, 128, 128)    -
G_synthesis/Grow_lod0         -         (?, 3, 128, 128)    -
G_synthesis/images_out        -         (?, 3, 128, 128)    -
G_synthesis/lod               -         ()                  -
G_synthesis/noise0            -         (1, 1, 4, 4)        -
G_synthesis/noise1            -         (1, 1, 4, 4)        -
G_synthesis/noise2            -         (1, 1, 8, 8)        -
G_synthesis/noise3            -         (1, 1, 8, 8)        -
G_synthesis/noise4            -         (1, 1, 16, 16)      -
G_synthesis/noise5            -         (1, 1, 16, 16)      -
G_synthesis/noise6            -         (1, 1, 32, 32)      -
G_synthesis/noise7            -         (1, 1, 32, 32)      -
G_synthesis/noise8            -         (1, 1, 64, 64)      -
G_synthesis/noise9            -         (1, 1, 64, 64)      -
G_synthesis/noise10           -         (1, 1, 128, 128)    -
G_synthesis/noise11           -         (1, 1, 128, 128)    -
images_out                    -         (?, 3, 128, 128)    -
---                           ---       ---                 ---
Total                         25843858


D                    Params    OutputShape         WeightShape
---                  ---       ---                 ---
images_in            -         (?, 3, 128, 128)    -
labels_in            -         (?, 0)              -
lod                  -         ()                  -
FromRGB_lod0         512       (?, 128, 128, 128)  (1, 1, 3, 128)
128x128/Conv0        147584    (?, 128, 128, 128)  (3, 3, 128, 128)
128x128/Conv1_down   295168    (?, 256, 64, 64)    (3, 3, 128, 256)
Downscale2D          -         (?, 3, 64, 64)      -
FromRGB_lod1         1024      (?, 256, 64, 64)    (1, 1, 3, 256)
Grow_lod0            -         (?, 256, 64, 64)    -
64x64/Conv0          590080    (?, 256, 64, 64)    (3, 3, 256, 256)
64x64/Conv1_down     1180160   (?, 512, 32, 32)    (3, 3, 256, 512)
Downscale2D_1        -         (?, 3, 32, 32)      -
FromRGB_lod2         2048      (?, 512, 32, 32)    (1, 1, 3, 512)
Grow_lod1            -         (?, 512, 32, 32)    -
32x32/Conv0          2359808   (?, 512, 32, 32)    (3, 3, 512, 512)
32x32/Conv1_down     2359808   (?, 512, 16, 16)    (3, 3, 512, 512)
Downscale2D_2        -         (?, 3, 16, 16)      -
FromRGB_lod3         2048      (?, 512, 16, 16)    (1, 1, 3, 512)
Grow_lod2            -         (?, 512, 16, 16)    -
16x16/Conv0          2359808   (?, 512, 16, 16)    (3, 3, 512, 512)
16x16/Conv1_down     2359808   (?, 512, 8, 8)      (3, 3, 512, 512)
Downscale2D_3        -         (?, 3, 8, 8)        -
FromRGB_lod4         2048      (?, 512, 8, 8)      (1, 1, 3, 512)
Grow_lod3            -         (?, 512, 8, 8)      -
8x8/Conv0            2359808   (?, 512, 8, 8)      (3, 3, 512, 512)
8x8/Conv1_down       2359808   (?, 512, 4, 4)      (3, 3, 512, 512)
Downscale2D_4        -         (?, 3, 4, 4)        -
FromRGB_lod5         2048      (?, 512, 4, 4)      (1, 1, 3, 512)
Grow_lod4            -         (?, 512, 4, 4)      -
4x4/MinibatchStddev  -         (?, 513, 4, 4)      -
4x4/Conv             2364416   (?, 512, 4, 4)      (3, 3, 513, 512)
4x4/Dense0           4194816   (?, 512)            (8192, 512)
4x4/Dense1           513       (?, 1)              (512, 1)
scores_out           -         (?, 1)              -
---                  ---       ---                 ---
Total                22941313

Building TensorFlow graph...
WARNING:tensorflow:From C:\Users\user\stylegan-master\training\training_loop.py:167: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
WARNING:tensorflow:From C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.

WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

Setting up snapshot image grid...
2020-01-20 07:05:17.296825: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-20 07:05:17.320746: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-20 07:05:17.342289: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.15GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-20 07:05:17.350675: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-20 07:05:17.399302: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.10GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
Setting up run dir...
Training...

2020-01-20 07:05:35.259782: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-20 07:05:35.316821: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-20 07:05:35.386177: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-20 07:05:35.430917: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-01-20 07:05:35.476293: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
tick 1     kimg 140.0    lod 4.00  minibatch 64   time 8m 55s       sec/tick 483.6   sec/kimg 3.45    maintenance 51.7   gpumem 1.6
network-snapshot-000140        time 8m 46s       fid50k 360.7307
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
C:\Users\user\Anaconda3\envs\keras-gpu\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
tick 2     kimg 280.1    lod 4.00  minibatch 64   time 25m 58s      sec/tick 479.1   sec/kimg 3.42    maintenance 543.3  gpumem 2.0
tick 3     kimg 420.1    lod 4.00  minibatch 64   time 33m 59s      sec/tick 479.0   sec/kimg 3.42    maintenance 2.3    gpumem 2.0
tick 4     kimg 560.1    lod 4.00  minibatch 64   time 42m 01s      sec/tick 479.8   sec/kimg 3.43    maintenance 2.2    gpumem 2.0
tick 5     kimg 680.2    lod 3.87  minibatch 32   time 59m 14s      sec/tick 1030.6  sec/kimg 8.58    maintenance 2.2    gpumem 2.0
tick 6     kimg 800.3    lod 3.67  minibatch 32   time 1h 21m 24s   sec/tick 1327.7  sec/kimg 11.06   maintenance 2.2    gpumem 2.0
tick 7     kimg 920.3    lod 3.47  minibatch 32   time 1h 43m 29s   sec/tick 1323.3  sec/kimg 11.02   maintenance 2.2    gpumem 2.0
tick 8     kimg 1040.4   lod 3.27  minibatch 32   time 2h 05m 23s   sec/tick 1311.2  sec/kimg 10.92   maintenance 2.2    gpumem 2.0
tick 9     kimg 1160.4   lod 3.07  minibatch 32   time 2h 27m 16s   sec/tick 1311.7  sec/kimg 10.92   maintenance 2.2    gpumem 2.0
tick 10    kimg 1280.5   lod 3.00  minibatch 32   time 2h 48m 55s   sec/tick 1296.9  sec/kimg 10.80   maintenance 2.2    gpumem 2.0
network-snapshot-001280        time 9m 16s       fid50k 292.2210
tick 11    kimg 1400.6   lod 3.00  minibatch 32   time 3h 19m 47s   sec/tick 1291.2  sec/kimg 10.75   maintenance 560.0  gpumem 2.0
tick 12    kimg 1520.6   lod 3.00  minibatch 32   time 3h 41m 20s   sec/tick 1291.5  sec/kimg 10.76   maintenance 2.2    gpumem 2.0
tick 13    kimg 1640.7   lod 3.00  minibatch 32   time 4h 02m 53s   sec/tick 1290.3  sec/kimg 10.75   maintenance 2.3    gpumem 2.0
tick 14    kimg 1760.8   lod 3.00  minibatch 32   time 4h 24m 26s   sec/tick 1290.8  sec/kimg 10.75   maintenance 2.2    gpumem 2.0
tick 15    kimg 1860.8   lod 2.90  minibatch 16   time 5h 08m 55s   sec/tick 2667.1  sec/kimg 26.66   maintenance 2.2    gpumem 2.0
tick 16    kimg 1960.8   lod 2.73  minibatch 16   time 6h 10m 02s   sec/tick 3663.8  sec/kimg 36.63   maintenance 3.3    gpumem 2.0
tick 17    kimg 2060.9   lod 2.57  minibatch 16   time 7h 11m 09s   sec/tick 3663.3  sec/kimg 36.62   maintenance 3.3    gpumem 2.0
tick 18    kimg 2160.9   lod 2.40  minibatch 16   time 8h 12m 15s   sec/tick 3663.3  sec/kimg 36.62   maintenance 3.3    gpumem 2.0
tick 19    kimg 2260.9   lod 2.23  minibatch 16   time 9h 13m 22s   sec/tick 3663.0  sec/kimg 36.62   maintenance 3.3    gpumem 2.0
tick 20    kimg 2361.0   lod 2.07  minibatch 16   time 10h 14m 28s  sec/tick 3662.6  sec/kimg 36.61   maintenance 3.3    gpumem 2.0
network-snapshot-002360        time 11m 20s      fid50k 329.8881
tick 21    kimg 2461.0   lod 2.00  minibatch 16   time 11h 26m 28s  sec/tick 3635.4  sec/kimg 36.34   maintenance 685.2  gpumem 2.0
tick 22    kimg 2561.0   lod 2.00  minibatch 16   time 12h 27m 40s  sec/tick 3668.3  sec/kimg 36.67   maintenance 3.3    gpumem 2.0
tick 23    kimg 2661.1   lod 2.00  minibatch 16   time 13h 28m 13s  sec/tick 3630.0  sec/kimg 36.29   maintenance 3.4    gpumem 2.0
tick 24    kimg 2761.1   lod 2.00  minibatch 16   time 14h 29m 10s  sec/tick 3652.9  sec/kimg 36.52   maintenance 3.4    gpumem 2.0
tick 25    kimg 2861.1   lod 2.00  minibatch 16   time 15h 29m 52s  sec/tick 3639.3  sec/kimg 36.38   maintenance 3.3    gpumem 2.0
tick 26    kimg 2961.2   lod 2.00  minibatch 16   time 16h 30m 13s  sec/tick 3617.6  sec/kimg 36.16   maintenance 3.3    gpumem 2.0
tick 27    kimg 3041.2   lod 1.93  minibatch 8    time 18h 07m 10s  sec/tick 5814.1  sec/kimg 72.68   maintenance 3.3    gpumem 2.0
tick 28    kimg 3121.2   lod 1.80  minibatch 8    time 20h 29m 23s  sec/tick 8525.3  sec/kimg 106.57  maintenance 7.0    gpumem 2.0
tick 29    kimg 3201.2   lod 1.66  minibatch 8    time 22h 51m 39s  sec/tick 8528.9  sec/kimg 106.61  maintenance 7.2    gpumem 2.0
tick 30    kimg 3281.2   lod 1.53  minibatch 8    time 1d 01h 14m   sec/tick 8536.7  sec/kimg 106.71  maintenance 7.3    gpumem 2.0
network-snapshot-003281        time 14m 53s      fid50k 321.2979
tick 31    kimg 3361.2   lod 1.40  minibatch 8    time 1d 03h 51m   sec/tick 8535.0  sec/kimg 106.69  maintenance 902.6  gpumem 2.0
tick 32    kimg 3441.2   lod 1.26  minibatch 8    time 1d 06h 13m   sec/tick 8542.2  sec/kimg 106.78  maintenance 7.4    gpumem 2.0
tick 33    kimg 3521.2   lod 1.13  minibatch 8    time 1d 08h 36m   sec/tick 8540.9  sec/kimg 106.76  maintenance 7.6    gpumem 2.0
tick 34    kimg 3601.2   lod 1.00  minibatch 8    time 1d 10h 58m   sec/tick 8538.5  sec/kimg 106.73  maintenance 7.5    gpumem 2.0
tick 35    kimg 3681.2   lod 1.00  minibatch 8    time 1d 13h 19m   sec/tick 8427.5  sec/kimg 105.34  maintenance 7.5    gpumem 2.0
．．．
</pre></div></div>
</div></div></section><div class="it-Footer"><div class="it-Footer_actions"><div class="it-Footer_editRequest"><a href="/drafts/aec7feabaa2f738ea82c/edit" class="u-link-no-underline"><span>編集リクエスト</span></a></div><div class="it-Footer_stock"><button><svg size="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 353.02 398" class="StockIcon__Stock-cq5opj-0 fEysOa"><path d="M176.72 398c-67.52 0-130.16-29-171.84-79.69l-4.46-5.42V78.05H353v234.84l-4.45 5.42C306.88 369 244.24 398 176.72 398zm-137.2-99.34c34.17 38.37 83.78 60.25 137.2 60.25s103-21.88 137.21-60.25V117.14H39.52zM0 0h351.12v40.94H0z"></path></svg><span class="it-Footer_stockLabel">ストック</span></button></div><div class="it-Footer_like" title="Looks Good To Me!"><button style="background-color:#fff"><span style="width:100%;height:100%;display:flex;align-items:center;justify-content:center"><svg size="56" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 564.81 145.68" color="#55C500" class="LgtmIcon__Lgtm-sc-1e4ee48-0 ZHnTl"><path d="M0 3.85h38.39v106.5h64.16v32H0zM195.41 66.82h71.82q.91 17.84-4 32.35A68.89 68.89 0 01248.71 124a65.49 65.49 0 01-22.9 16 74.08 74.08 0 01-29.17 5.65 78.16 78.16 0 01-54.17-21.32 74.1 74.1 0 01-16.42-23.21 68.08 68.08 0 01-6-28.41 67.7 67.7 0 016-28.42 74.44 74.44 0 0116.42-23.1 76.37 76.37 0 0124.32-15.52 78.92 78.92 0 0184.43 16.06l-25 25A36.52 36.52 0 00213.57 36a37.65 37.65 0 00-31.83-.87 37.7 37.7 0 00-12.08 8.57 40.75 40.75 0 00-8.17 12.91 42.8 42.8 0 00-3 16.22 42.91 42.91 0 003 16.17 40.44 40.44 0 008.17 13 37.4 37.4 0 0012.08 8.57 36.07 36.07 0 0014.9 3.11q14.38 0 22.13-5.35a25.46 25.46 0 0010.31-14.65h-33.77zM319.44 36.21h-36.55v-32h111.18v32h-36.45v106.5h-38.18zM486.82 109l-29.73-44.18v77.89H418.7V4.24h33.83l39 55.09 39.86-55.09h33.42v138.47h-38.38V65.23L496.19 109z"></path></svg></span></button><a href="/MuAuan/items/aec7feabaa2f738ea82c/likers" class="it-Footer_likeCount">2</a></div></div><div class="it-Footer_social"><div class="it-Footer_shareButton it-Footer_shareButton-twitter"><span class="fa fa-twitter"></span></div><div class="it-Footer_shareButton it-Footer_shareButton-facebook"><span class="fa fa-facebook"></span></div></div></div><div class="ai-Container"><div class="ai-User"><a href="/MuAuan"><img src="https://qiita-user-profile-images.imgix.net/https%3A%2F%2Favatars2.githubusercontent.com%2Fu%2F35788955%3Fv%3D4?ixlib=rb-1.2.2&amp;auto=compress%2Cformat&amp;lossless=0&amp;w=75&amp;s=e420ddb9abfbecae939e655562343f59" alt="MuAuan" class="ai-User_image"/></a><div class="ai-User_body"><div class="ai-User_header"><a href="/MuAuan" class="ai-User_name">ムー ウワン</a><a href="/MuAuan" class="ai-User_urlname">@<!-- -->MuAuan</a></div><div class="ai-User_description">2020年目標；いい記事を書く

記事350いいね1500フォロワー150
2019年の実績／目標
記事275／300いいね1035／1000フォロワー97／100
1/7/2019
記事219いいね784フォロワー76
2018年の実績／目標
記事140／200いいね423／500フォロワー48／50
7/8/2018
記事90いいね227フォロワー25</div><div class="ai-User_footer"><button class="UserFollowButton-sc-1hmm0rc-0 iytOeR">フォロー</button></div></div></div></div></div></div><div class="p-items_options"><div class="mt-2"></div></div><div class="p-items_toc"><div class="mt-2"></div></div></div></div><div class="p-items_wrapper p-items_wrapper-white"><div class="p-items_container"><div class="p-items_leftDummy"></div><div class="p-items_main"><div class="p-items_aside px-5 p-2@s"><div id="logly-lift-4279494"><div class="tl-DummyItemList p-2"><div class="tl-DummyItemList_content tl-DummyItem"><div class="tl-DummyItem_image mr-1"></div><div class="tl-DummyItem_body"><div class="tl-DummyItem_text mb-1"></div><div class="tl-DummyItem_text mb-1"></div><div class="tl-DummyItem_text mb-1"></div></div></div><div class="tl-DummyItemList_content tl-DummyItem"><div class="tl-DummyItem_image mr-1"></div><div class="tl-DummyItem_body"><div class="tl-DummyItem_text mb-1"></div><div class="tl-DummyItem_text mb-1"></div><div class="tl-DummyItem_text mb-1"></div></div></div><div class="tl-DummyItemList_content tl-DummyItem"><div class="tl-DummyItem_image mr-1"></div><div class="tl-DummyItem_body"><div class="tl-DummyItem_text mb-1"></div><div class="tl-DummyItem_text mb-1"></div><div class="tl-DummyItem_text mb-1"></div></div></div><div class="tl-DummyItemList_content tl-DummyItem"><div class="tl-DummyItem_image mr-1"></div><div class="tl-DummyItem_body"><div class="tl-DummyItem_text mb-1"></div><div class="tl-DummyItem_text mb-1"></div><div class="tl-DummyItem_text mb-1"></div></div></div><div class="tl-DummyItemList_content tl-DummyItem"><div class="tl-DummyItem_image mr-1"></div><div class="tl-DummyItem_body"><div class="tl-DummyItem_text mb-1"></div><div class="tl-DummyItem_text mb-1"></div><div class="tl-DummyItem_text mb-1"></div></div></div></div></div></div><div class="p-items_aside mt-5 px-5 p-2@s"></div><div></div><div class="p-items_aside mt-6 px-5 p-2@s" id="comments-wrapper"><div id="comments" class="co-ItemWrapper"><div class="co-ItemWrapper_title mb-2"><span class="fa fa-comments mr-1"></span>コメント</div><div class="mb-4">この記事にコメントはありません。</div><div style="margin-top:30px"><div><div><div class="CommentForm__Header-xcwl8q-0 gnNGXY"><img src="https://qiita-user-profile-images.imgix.net/https%3A%2F%2Flh6.googleusercontent.com%2F-3EHSHCCSCmg%2FAAAAAAAAAAI%2FAAAAAAAAAAA%2FAKF05nAVp3wix0GPOS2jqZ7xSDRYCDF6Mw%2Fs50%2Fphoto.jpg?ixlib=rb-1.2.2&amp;auto=compress%2Cformat&amp;lossless=0&amp;w=48&amp;s=be4e5cacc9e0864ac1ef96d37914a7b9" width="36" height="36" class="Avatar__Image-sc-1u4xwgc-0 hGIfZz"/><span class="CommentForm__HeaderTitle-xcwl8q-1 eZJAwV">コメントを投稿する</span></div><div class="CommentForm__ContentHeader-xcwl8q-2 fZEEYf"><div class="CommentForm__ContentHeaderTabs-xcwl8q-3 lnidcq"><div class="CommentForm__ContentHeaderTabsItem-xcwl8q-4 esvsAC">編集</div><div class="CommentForm__ContentHeaderTabsItem-xcwl8q-4 joOXXL">プレビュー</div></div><div class="CommentForm__ContentHeaderToolbar-xcwl8q-5 jkYbvp"><a href="https://qiita.com/Qiita/items/c686397e4a0f4f11683d" target="_blank" title="Markdownの書き方" class="CommentForm__ContentHeaderToolbarLink-xcwl8q-7 QXFlv"><span class="fa fa-question-circle fa-lg"></span></a><div title="絵文字を選択" class="CommentForm__ContentHeaderToolbarItem-xcwl8q-6 eYFchP"><span class="fa fa-smile-o fa-lg"></span></div></div></div><div class="CommentForm__Body-xcwl8q-10 dkpzIG"><textarea placeholder="コメントを入力" class="CommentForm__CommentFormTextarea-xcwl8q-12 fEfHfr"></textarea><div style="display:none;margin-bottom:0" class="CommentForm__BodyPreview-xcwl8q-11 drBRWd co-Item_text"><div></div></div></div><div class="CommentForm__Footer-xcwl8q-13 XemJY"><div class="CommentForm__Uploader-xcwl8q-14 hEjQqy"><span class="CommentForm__UploaderButton-xcwl8q-15 hZWjpt"><i class="fa fa-picture-o"></i>画像を選択</span><span title="アップロード可能なサイズ" class="CommentForm__UploaderText-xcwl8q-16 bMzayN">0B / 100MB</span></div><button style="margin-left:auto" class="Button-sc-998iob-0 hEwpMS" color="primary">投稿</button></div><input type="file" style="display:none"/></div></div></div></div></div></div><div class="p-items_rightDummy"></div></div></div></div></div>
      <script type="application/json" class="js-react-on-rails-component" data-component-name="PersonalArticle" data-dom-id="PersonalArticle-react-component-b6c0f11d-cc2d-4c61-9a57-30cd0d392729">{"article":{"body":"\u003cp\u003e今回はStyleGANを使ってアニメ顔の学習に挑戦してみました。\u003cbr\u003e\n学習に関する参考はほとんどなく以下のものがありました。結構学習に時間もかかるし、情報も不十分でしたが、一応自前マシンで学習でき、かつ学習途中からの再学習もできたので、記事にまとめておきます。\u003cbr\u003e\n【参考】\u003cbr\u003e\n①\u003ca href=\"https://evigio.com/post/how-to-use-custom-datasets-with-stylegan-tensorFlow-implementation\" rel=\"nofollow noopener\" target=\"_blank\"\u003eHow To Use Custom Datasets With StyleGAN - TensorFlow Implementation\u003c/a\u003e\u003cbr\u003e\n②\u003ca href=\"http://blog.livedoor.jp/tak_tak0/archives/52409271.html\" rel=\"nofollow noopener\" target=\"_blank\"\u003estyleganで独自モデルの学習方法\u003c/a\u003e\u003cbr\u003e\n③\u003ca href=\"https://gist.github.com/eukaryote31/e7406e49f62f23e4004afa9788cafaa0\" rel=\"nofollow noopener\" target=\"_blank\"\u003eStyleGAN log\u003c/a\u003e\u003cbr\u003e\n④\u003ca href=\"https://www.gwern.net/Faces\" rel=\"nofollow noopener\" target=\"_blank\"\u003eMaking Anime Faces With StyleGAN\u003c/a\u003e\u003c/p\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"やったこと\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%84%E3%81%A3%E3%81%9F%E3%81%93%E3%81%A8\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eやったこと\u003c/h3\u003e\n\n\u003cp\u003e・アニメ顔データの準備\u003cbr\u003e\n・とにかく学習する\u003cbr\u003e\n・潜在空間でのミキシングをやってみる\u003cbr\u003e\n・再学習するには\u003c/p\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"アニメ顔データの準備\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%A2%E3%83%8B%E3%83%A1%E9%A1%94%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E6%BA%96%E5%82%99\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e・アニメ顔データの準備\u003c/h3\u003e\n\n\u003cp\u003eアニメ顔は\u003ca href=\"https://qiita.com/MuAuan/items/85db7176574bdf979061\" id=\"reference-c4df0e17f3243891c3e1\"\u003e以前DCGANで利用したサイト\u003c/a\u003eから、ダウンロードして準備しました。\u003cbr\u003e\n今回の利用のポイントは少なくとも画像サイズを合わせて、かつファイル名を変更して読み取りやすく、1.pngのように変更すること。\u003cbr\u003e\nStyleGANの学習という意味では目鼻顔などのアラインメントを合わせたいが、次回にパスした。\u003cbr\u003e\nということで、上記のデータ整理は以下のコードで実施した。\u003cbr\u003e\nちなみにサイズ（128,128）を1000個用意した。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"py\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003ePIL\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eImage\u003c/span\u003e\n\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eglob\u003c/span\u003e\n\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003erandom\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003efiles\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eglob\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eglob\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"./anime/**/*.png\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003erecursive\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003efiles\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003erandom\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esample\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003efiles\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1000\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003eres\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[]\u003c/span\u003e\n\u003cspan class=\"n\"\u003esk\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\n\u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003epath\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003efiles\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eimg\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eImage\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nb\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eimg\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eimg\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eresize\u003c/span\u003e\u003cspan class=\"p\"\u003e((\u003c/span\u003e\u003cspan class=\"mi\"\u003e128\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e128\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eimg\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esave\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e \u003cspan class=\"s\"\u003e\"img/{}.png\"\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nb\"\u003eformat\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esk\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n    \u003cspan class=\"n\"\u003esk\u003c/span\u003e \u003cspan class=\"o\"\u003e+=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"とにかく学習する\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%A8%E3%81%AB%E3%81%8B%E3%81%8F%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e・とにかく学習する\u003c/h3\u003e\n\n\u003cp\u003e学習コードは、参考①のビデオと参考②を眺めつつ、以下のようにした。\u003c/p\u003e\n\n\u003cp\u003ekimgは学習image数であり、単位が千imgを意味している。\u003cbr\u003e\n総学習イメージ数が3400kimgを意味する\u003cbr\u003e\n次の行は学習開始解像度＝４\u003cbr\u003e\nそして、custom_datasetがtf_recordsに変換した画像のDir(＝datasets/custom_dataset)である。さらに、とりあえずの学習ということで、解像度=64で学習してみたが問題なく学習できた。\u003cbr\u003e\nまた、StyleGANはpganであり、サイズ毎学習であるが、サイズ毎のminibatchサイズも以下のとおり、小さめに設定した。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"py\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003cspan class=\"n\"\u003etrain\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etotal_kimg\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e3400\u003c/span\u003e\u003cspan class=\"err\"\u003e、\u003c/span\u003e\n\u003cspan class=\"n\"\u003esched\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elod_initial_resolution\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\n\u003cspan class=\"n\"\u003edesc\u003c/span\u003e \u003cspan class=\"o\"\u003e+=\u003c/span\u003e \u003cspan class=\"s\"\u003e'-custom_dataset'\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e     \u003cspan class=\"n\"\u003edataset\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eEasyDict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etfrecord_dir\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'custom_dataset'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eresolution\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e64\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e                 \u003cspan class=\"n\"\u003etrain\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emirror_augment\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eFalse\u003c/span\u003e\n\u003cspan class=\"n\"\u003edesc\u003c/span\u003e \u003cspan class=\"o\"\u003e+=\u003c/span\u003e \u003cspan class=\"s\"\u003e'-1gpu'\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"n\"\u003esubmit_config\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enum_gpus\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"n\"\u003esched\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eminibatch_base\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"n\"\u003esched\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eminibatch_dict\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e128\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e64\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e16\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e32\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e32\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e16\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e64\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e128\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e256\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e512\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eつまり、以下の必要最小限で動く。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"python\"\u003e\n\u003cdiv class=\"code-lang\"\u003e\u003cspan class=\"bold\"\u003etrain.py\u003c/span\u003e\u003c/div\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003cspan class=\"c1\"\u003e# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\u003c/span\u003e\n\u003cspan class=\"s\"\u003e\"\"\"Main entry point for training StyleGAN and ProGAN networks.\"\"\"\u003c/span\u003e\n\n\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ecopy\u003c/span\u003e\n\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ednnlib\u003c/span\u003e\n\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003ednnlib\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eEasyDict\u003c/span\u003e\n\n\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003econfig\u003c/span\u003e\n\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003emetrics\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003emetric_base\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e#----------------------------------------------------------------------------\n# Official training configs for StyleGAN, targeted mainly for FFHQ.\n\u003c/span\u003e\n\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"n\"\u003edesc\u003c/span\u003e          \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e'sgan'\u003c/span\u003e                                                                 \u003cspan class=\"c1\"\u003e# Description string included in result subdir name.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003etrain\u003c/span\u003e         \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eEasyDict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erun_func_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'training.training_loop.training_loop'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e         \u003cspan class=\"c1\"\u003e# Options for training loop.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eG\u003c/span\u003e             \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eEasyDict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003efunc_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'training.networks_stylegan.G_style'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e               \u003cspan class=\"c1\"\u003e# Options for generator network.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eD\u003c/span\u003e             \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eEasyDict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003efunc_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'training.networks_stylegan.D_basic'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e               \u003cspan class=\"c1\"\u003e# Options for discriminator network.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eG_opt\u003c/span\u003e         \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eEasyDict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ebeta1\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mf\"\u003e0.0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ebeta2\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mf\"\u003e0.99\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eepsilon\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mf\"\u003e1e-8\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e                          \u003cspan class=\"c1\"\u003e# Options for generator optimizer.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eD_opt\u003c/span\u003e         \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eEasyDict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ebeta1\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mf\"\u003e0.0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ebeta2\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mf\"\u003e0.99\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eepsilon\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mf\"\u003e1e-8\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e                          \u003cspan class=\"c1\"\u003e# Options for discriminator optimizer.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eG_loss\u003c/span\u003e        \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eEasyDict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003efunc_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'training.loss.G_logistic_nonsaturating'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e           \u003cspan class=\"c1\"\u003e# Options for generator loss.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eD_loss\u003c/span\u003e        \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eEasyDict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003efunc_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'training.loss.D_logistic_simplegp'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003er1_gamma\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mf\"\u003e10.0\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"c1\"\u003e# Options for discriminator loss.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003edataset\u003c/span\u003e       \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eEasyDict\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e                                                             \u003cspan class=\"c1\"\u003e# Options for load_dataset().\n\u003c/span\u003e    \u003cspan class=\"n\"\u003esched\u003c/span\u003e         \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eEasyDict\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e                                                             \u003cspan class=\"c1\"\u003e# Options for TrainingSchedule.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003egrid\u003c/span\u003e          \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eEasyDict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'4k'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003elayout\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'random'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e  \u003cspan class=\"c1\"\u003e#4k                              # Options for setup_snapshot_image_grid().\n\u003c/span\u003e    \u003cspan class=\"n\"\u003emetrics\u003c/span\u003e       \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003emetric_base\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efid50k\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e                                                   \u003cspan class=\"c1\"\u003e# Options for MetricGroup.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003esubmit_config\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ednnlib\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eSubmitConfig\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e                                                  \u003cspan class=\"c1\"\u003e# Options for dnnlib.submit_run().\n\u003c/span\u003e    \u003cspan class=\"n\"\u003etf_config\u003c/span\u003e     \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s\"\u003e'rnd.np_random_seed'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e1000\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e                                           \u003cspan class=\"c1\"\u003e# Options for tflib.init_tf().\n\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e# Dataset.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003edesc\u003c/span\u003e \u003cspan class=\"o\"\u003e+=\u003c/span\u003e \u003cspan class=\"s\"\u003e'-custom_dataset'\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e     \u003cspan class=\"n\"\u003edataset\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eEasyDict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etfrecord_dir\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'custom_dataset'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eresolution\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e64\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e                 \u003cspan class=\"n\"\u003etrain\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emirror_augment\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eFalse\u003c/span\u003e\n\n    \u003cspan class=\"c1\"\u003e# Number of GPUs.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003edesc\u003c/span\u003e \u003cspan class=\"o\"\u003e+=\u003c/span\u003e \u003cspan class=\"s\"\u003e'-1gpu'\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"n\"\u003esubmit_config\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enum_gpus\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"n\"\u003esched\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eminibatch_base\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"n\"\u003esched\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eminibatch_dict\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e128\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e64\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e16\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e32\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e32\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e16\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e64\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e128\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e256\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e512\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n    \u003cspan class=\"c1\"\u003e# Default options.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003etrain\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etotal_kimg\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e3400\u003c/span\u003e\n    \u003cspan class=\"n\"\u003esched\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elod_initial_resolution\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\n    \u003cspan class=\"n\"\u003esched\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eG_lrate_dict\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"mi\"\u003e128\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.0015\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e256\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.002\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e512\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.003\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1024\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.003\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \u003cspan class=\"n\"\u003esched\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eD_lrate_dict\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eEasyDict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esched\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eG_lrate_dict\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e#----------------------------------------------------------------------------\n# Main entry point for training.\n# Calls the function indicated by 'train' using the selected options.\n\u003c/span\u003e\n\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eEasyDict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etrain\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eupdate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eG_args\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eG\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eD_args\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eD\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eG_opt_args\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eG_opt\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eD_opt_args\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eD_opt\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eG_loss_args\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eG_loss\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eD_loss_args\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eD_loss\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eupdate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edataset_args\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003edataset\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003esched_args\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003esched\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003egrid_args\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003egrid\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003emetric_arg_list\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003emetrics\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etf_config\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003etf_config\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esubmit_config\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ecopy\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edeepcopy\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esubmit_config\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esubmit_config\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun_dir_root\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ednnlib\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esubmission\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esubmit\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eget_template_from_path\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003econfig\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eresult_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esubmit_config\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun_dir_ignore\u003c/span\u003e \u003cspan class=\"o\"\u003e+=\u003c/span\u003e \u003cspan class=\"n\"\u003econfig\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun_dir_ignore\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esubmit_config\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun_desc\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edesc\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ednnlib\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esubmit_run\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e#----------------------------------------------------------------------------\n\u003c/span\u003e\n\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003e__name__\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s\"\u003e\"__main__\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"n\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e#----------------------------------------------------------------------------\n\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003c/div\u003e\n\n\u003cp\u003eなお、データのtfrecordsへの変換は参考①から以下のように実施した。\u003cbr\u003e\n※ここで元画像は./animeに入れておく、そして画像サイズ毎の変換ファイルはcustom_datasetに６個のサイズの異なるファイルが格納された。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"text\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003epython dataset_tool.py create_from_images datasets/custom_dataset ./anime\n\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e上記でとりあえず、学習できると思う。\u003c/p\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"潜在空間でのミキシングをやってみる\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E6%BD%9C%E5%9C%A8%E7%A9%BA%E9%96%93%E3%81%A7%E3%81%AE%E3%83%9F%E3%82%AD%E3%82%B7%E3%83%B3%E3%82%B0%E3%82%92%E3%82%84%E3%81%A3%E3%81%A6%E3%81%BF%E3%82%8B\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e・潜在空間でのミキシングをやってみる\u003c/h3\u003e\n\n\u003cp\u003e1060マシンで上記のコードを10h程度で以下の絵が得られる。\u003cbr\u003e\n決して綺麗とは言えないが、とにかく最弱マシンでも学習できた。\u003cbr\u003e\n\u003ca href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F5a66106f-1906-e5e3-718d-402b320f922b.jpeg?ixlib=rb-1.2.2\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=d785fce7148ed01bf434160828677a1c\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F5a66106f-1906-e5e3-718d-402b320f922b.jpeg?ixlib=rb-1.2.2\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=d785fce7148ed01bf434160828677a1c\" alt=\"64x64_3400.jpg\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/233744/5a66106f-1906-e5e3-718d-402b320f922b.jpeg\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F5a66106f-1906-e5e3-718d-402b320f922b.jpeg?ixlib=rb-1.2.2\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=3d3c709f538fc75a5bec571e5086bd7d 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003cbr\u003e\nさらに、潜在空間での17，18のミキシングをやってみると以下の絵が得られた。\u003cbr\u003e\n\u003ca href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F99bb4f44-d8d1-6ca7-aa65-08f087cf9601.gif?ixlib=rb-1.2.2\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=a6f3e8224cbacc15f006258effd8fbb1\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F99bb4f44-d8d1-6ca7-aa65-08f087cf9601.gif?ixlib=rb-1.2.2\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=a6f3e8224cbacc15f006258effd8fbb1\" alt=\"example17_18.gif\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/233744/99bb4f44-d8d1-6ca7-aa65-08f087cf9601.gif\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F99bb4f44-d8d1-6ca7-aa65-08f087cf9601.gif?ixlib=rb-1.2.2\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=5a9695722864ecaa3da7cf176a03babb 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003cbr\u003e\n1080マシンで解像度128x128サイズで1d8h程度回して、kimg=4705の場合以下のように画像がしっかりしてきた。\u003cbr\u003e\n※これでもpid50K=168程度で精度はまだまだだが、。。。\u003ca href=\"https://qiita.com/MuAuan/items/85db7176574bdf979061\"\u003e以前のDCGANの画像\u003c/a\u003eと比べてこちらの方が綺麗に見える\u003cbr\u003e\n\u003ca href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F6ce25027-b303-4de4-83fc-3909c57bf178.jpeg?ixlib=rb-1.2.2\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=a526b17da1157b8e799a6c003a7d8f6e\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F6ce25027-b303-4de4-83fc-3909c57bf178.jpeg?ixlib=rb-1.2.2\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=a526b17da1157b8e799a6c003a7d8f6e\" alt=\"128x128_4705_25.jpg\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/233744/6ce25027-b303-4de4-83fc-3909c57bf178.jpeg\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F6ce25027-b303-4de4-83fc-3909c57bf178.jpeg?ixlib=rb-1.2.2\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=e54051ebfe78c5d3c2d53e4f103c96e7 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003cbr\u003e\nさらに、潜在空間での11，82のミキシングをやってみると以下の絵が得られた。\u003cbr\u003e\n\u003ca href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F93df08de-f7a7-25d7-90ca-91a248fd84f2.gif?ixlib=rb-1.2.2\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=86bf6c81b791eba87c980cda003886da\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F93df08de-f7a7-25d7-90ca-91a248fd84f2.gif?ixlib=rb-1.2.2\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=86bf6c81b791eba87c980cda003886da\" alt=\"example_128_4705_100_82x11.gif\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/233744/93df08de-f7a7-25d7-90ca-91a248fd84f2.gif\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F233744%2F93df08de-f7a7-25d7-90ca-91a248fd84f2.gif?ixlib=rb-1.2.2\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=0954e31f6c5e1e16d85c815c15ad2244 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"再学習するには\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%86%8D%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B%E3%81%AB%E3%81%AF\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e・再学習するには\u003c/h3\u003e\n\n\u003cp\u003e最後に禁断（誰も公開していないようなので）の途中中断した場合の継続学習の仕方が出来たので、まとめておく。\u003cbr\u003e\n※この方法はちょっとだけ参考②と参考④に記載がある\u003cbr\u003e\n以下のコードで実施する。\u003cbr\u003e\nすなわち、training_loop.pyの以下の部分を修正する。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"py\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e    \u003cspan class=\"n\"\u003eresume_run_id\u003c/span\u003e           \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\"latest\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"c1\"\u003e#None,     # Run ID or network pkl to resume training from, None = start from scratch.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eresume_snapshot\u003c/span\u003e         \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e'./results/00001-sgan-custom_dataset-1gpu/network-snapshot-.pkl'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"c1\"\u003e#None,     # Snapshot index to resume training from, None = autodetect.\n\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eまた、network_snapshot_ticks  = 1,       # How often to export network snapshots? として、毎回出力にしている。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"python\"\u003e\n\u003cdiv class=\"code-lang\"\u003e\u003cspan class=\"bold\"\u003etraining_loop.py\u003c/span\u003e\u003c/div\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003etraining_loop\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n    \u003cspan class=\"n\"\u003esubmit_config\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eG_args\u003c/span\u003e                  \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{},\u003c/span\u003e       \u003cspan class=\"c1\"\u003e# Options for generator network.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eD_args\u003c/span\u003e                  \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{},\u003c/span\u003e       \u003cspan class=\"c1\"\u003e# Options for discriminator network.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eG_opt_args\u003c/span\u003e              \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{},\u003c/span\u003e       \u003cspan class=\"c1\"\u003e# Options for generator optimizer.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eD_opt_args\u003c/span\u003e              \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{},\u003c/span\u003e       \u003cspan class=\"c1\"\u003e# Options for discriminator optimizer.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eG_loss_args\u003c/span\u003e             \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{},\u003c/span\u003e       \u003cspan class=\"c1\"\u003e# Options for generator loss.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eD_loss_args\u003c/span\u003e             \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{},\u003c/span\u003e       \u003cspan class=\"c1\"\u003e# Options for discriminator loss.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003edataset_args\u003c/span\u003e            \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{},\u003c/span\u003e       \u003cspan class=\"c1\"\u003e# Options for dataset.load_dataset().\n\u003c/span\u003e    \u003cspan class=\"n\"\u003esched_args\u003c/span\u003e              \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{},\u003c/span\u003e       \u003cspan class=\"c1\"\u003e# Options for train.TrainingSchedule.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003egrid_args\u003c/span\u003e               \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{},\u003c/span\u003e       \u003cspan class=\"c1\"\u003e# Options for train.setup_snapshot_image_grid().\n\u003c/span\u003e    \u003cspan class=\"n\"\u003emetric_arg_list\u003c/span\u003e         \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[],\u003c/span\u003e       \u003cspan class=\"c1\"\u003e# Options for MetricGroup.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003etf_config\u003c/span\u003e               \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{},\u003c/span\u003e       \u003cspan class=\"c1\"\u003e# Options for tflib.init_tf().\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eG_smoothing_kimg\u003c/span\u003e        \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mf\"\u003e10.0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e     \u003cspan class=\"c1\"\u003e# Half-life of the running average of generator weights.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eD_repeats\u003c/span\u003e               \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e        \u003cspan class=\"c1\"\u003e# How many times the discriminator is trained per G iteration.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eminibatch_repeats\u003c/span\u003e       \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e        \u003cspan class=\"c1\"\u003e# Number of minibatches to run before adjusting training parameters.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003ereset_opt_for_new_lod\u003c/span\u003e   \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e     \u003cspan class=\"c1\"\u003e# Reset optimizer internal state (e.g. Adam moments) when new layers are introduced?\n\u003c/span\u003e    \u003cspan class=\"n\"\u003etotal_kimg\u003c/span\u003e              \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e15000\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e    \u003cspan class=\"c1\"\u003e# Total length of the training, measured in thousands of real images.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003emirror_augment\u003c/span\u003e          \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e    \u003cspan class=\"c1\"\u003e# Enable mirror augment?\n\u003c/span\u003e    \u003cspan class=\"n\"\u003edrange_net\u003c/span\u003e              \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e   \u003cspan class=\"c1\"\u003e# Dynamic range used when feeding image data to the networks.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eimage_snapshot_ticks\u003c/span\u003e    \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e        \u003cspan class=\"c1\"\u003e# How often to export image snapshots?\n\u003c/span\u003e    \u003cspan class=\"n\"\u003enetwork_snapshot_ticks\u003c/span\u003e  \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e       \u003cspan class=\"c1\"\u003e# How often to export network snapshots? default=10\n\u003c/span\u003e    \u003cspan class=\"n\"\u003esave_tf_graph\u003c/span\u003e           \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e    \u003cspan class=\"c1\"\u003e# Include full TensorFlow computation graph in the tfevents file?\n\u003c/span\u003e    \u003cspan class=\"n\"\u003esave_weight_histograms\u003c/span\u003e  \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e    \u003cspan class=\"c1\"\u003e# Include weight histograms in the tfevents file?\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eresume_run_id\u003c/span\u003e           \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\"latest\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"c1\"\u003e#None,     # Run ID or network pkl to resume training from, None = start from scratch.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eresume_snapshot\u003c/span\u003e         \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e'./results/00001-sgan-custom_dataset-1gpu/network-snapshot-.pkl'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"c1\"\u003e#None,     # Snapshot index to resume training from, None = autodetect.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eresume_kimg\u003c/span\u003e             \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mf\"\u003e1040.9\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e      \u003cspan class=\"c1\"\u003e# Assumed training progress at the beginning. Affects reporting and training schedule.\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eresume_time\u003c/span\u003e             \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mf\"\u003e5599.0\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e     \u003cspan class=\"c1\"\u003e# Assumed wallclock time at the beginning. Affects reporting.\n\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003c/div\u003e\n\n\u003cp\u003eresume_time             = 5599.0\u003cbr\u003e\nは秒単位で入力する。\u003cbr\u003e\nこのままだとメモリーがたまらないので、さらに上書き保存するためにもう一か所、下のコードのように変更した。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"python\"\u003e\n\u003cdiv class=\"code-lang\"\u003e\u003cspan class=\"bold\"\u003etrain_loops.py\u003c/span\u003e\u003c/div\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003ecur_tick\u003c/span\u003e \u003cspan class=\"o\"\u003e%\u003c/span\u003e \u003cspan class=\"n\"\u003enetwork_snapshot_ticks\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e \u003cspan class=\"ow\"\u003eor\u003c/span\u003e \u003cspan class=\"n\"\u003edone\u003c/span\u003e \u003cspan class=\"ow\"\u003eor\u003c/span\u003e \u003cspan class=\"n\"\u003ecur_tick\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n                \u003cspan class=\"c1\"\u003e#pkl = os.path.join(submit_config.run_dir, 'network-snapshot-%06d.pkl' % (cur_nimg // 1000))\n\u003c/span\u003e                \u003cspan class=\"n\"\u003epkl\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ejoin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esubmit_config\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s\"\u003e'network-snapshot-.pkl'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"n\"\u003emisc\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esave_pkl\u003c/span\u003e\u003cspan class=\"p\"\u003e((\u003c/span\u003e\u003cspan class=\"n\"\u003eG\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eD\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eGs\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"n\"\u003epkl\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"n\"\u003emetrics\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epkl\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003erun_dir\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003esubmit_config\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003enum_gpus\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003esubmit_config\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enum_gpus\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etf_config\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003etf_config\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e\n\u003c/div\u003e\n\n\u003cp\u003e計算が1080マシンでもかなりかかるので、これを利用した結果については後日公開したいと思う。\u003c/p\u003e\n\n\u003ch3\u003e\n\u003cspan id=\"まとめ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%BE%E3%81%A8%E3%82%81\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eまとめ\u003c/h3\u003e\n\n\u003cp\u003e・自前マシンでStyleGANの学習ができるようになった\u003cbr\u003e\n・学習データで今まで報告したようなミキシングが出来た\u003cbr\u003e\n・途中で中断した場合の再開の仕方が分かった\u003c/p\u003e\n\n\u003cp\u003e・今回は1000データだが、100以下の少数データの結果も見ようと思う\u003cbr\u003e\n・さらに精度を追求してスタイルなどもやってみようと思う\u003c/p\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"おまけ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%8A%E3%81%BE%E3%81%91\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eおまけ\u003c/h1\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"text\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003ednnlib: Running training.training_loop.training_loop() on localhost...\nStreaming data using training.dataset.TFRecordDataset...\nDataset shape = [3, 64, 64]\nDynamic range = [0, 255]\nLabel size    = 0\nConstructing networks...\n\nG                           Params    OutputShape       WeightShape     \n---                         ---       ---               ---             \nlatents_in                  -         (?, 512)          -               \nlabels_in                   -         (?, 0)            -               \nlod                         -         ()                -               \ndlatent_avg                 -         (512,)            -               \nG_mapping/latents_in        -         (?, 512)          -               \nG_mapping/labels_in         -         (?, 0)            -               \nG_mapping/PixelNorm         -         (?, 512)          -               \nG_mapping/Dense0            262656    (?, 512)          (512, 512)      \nG_mapping/Dense1            262656    (?, 512)          (512, 512)      \nG_mapping/Dense2            262656    (?, 512)          (512, 512)      \nG_mapping/Dense3            262656    (?, 512)          (512, 512)      \nG_mapping/Dense4            262656    (?, 512)          (512, 512)      \nG_mapping/Dense5            262656    (?, 512)          (512, 512)      \nG_mapping/Dense6            262656    (?, 512)          (512, 512)      \nG_mapping/Dense7            262656    (?, 512)          (512, 512)      \nG_mapping/Broadcast         -         (?, 10, 512)      -               \nG_mapping/dlatents_out      -         (?, 10, 512)      -               \nTruncation                  -         (?, 10, 512)      -               \nG_synthesis/dlatents_in     -         (?, 10, 512)      -               \nG_synthesis/4x4/Const       534528    (?, 512, 4, 4)    (512,)          \nG_synthesis/4x4/Conv        2885632   (?, 512, 4, 4)    (3, 3, 512, 512)\nG_synthesis/ToRGB_lod4      1539      (?, 3, 4, 4)      (1, 1, 512, 3)  \nG_synthesis/8x8/Conv0_up    2885632   (?, 512, 8, 8)    (3, 3, 512, 512)\nG_synthesis/8x8/Conv1       2885632   (?, 512, 8, 8)    (3, 3, 512, 512)\nG_synthesis/ToRGB_lod3      1539      (?, 3, 8, 8)      (1, 1, 512, 3)  \nG_synthesis/Upscale2D       -         (?, 3, 8, 8)      -               \nG_synthesis/Grow_lod3       -         (?, 3, 8, 8)      -               \nG_synthesis/16x16/Conv0_up  2885632   (?, 512, 16, 16)  (3, 3, 512, 512)\nG_synthesis/16x16/Conv1     2885632   (?, 512, 16, 16)  (3, 3, 512, 512)\nG_synthesis/ToRGB_lod2      1539      (?, 3, 16, 16)    (1, 1, 512, 3)  \nG_synthesis/Upscale2D_1     -         (?, 3, 16, 16)    -               \nG_synthesis/Grow_lod2       -         (?, 3, 16, 16)    -               \nG_synthesis/32x32/Conv0_up  2885632   (?, 512, 32, 32)  (3, 3, 512, 512)\nG_synthesis/32x32/Conv1     2885632   (?, 512, 32, 32)  (3, 3, 512, 512)\nG_synthesis/ToRGB_lod1      1539      (?, 3, 32, 32)    (1, 1, 512, 3)  \nG_synthesis/Upscale2D_2     -         (?, 3, 32, 32)    -               \nG_synthesis/Grow_lod1       -         (?, 3, 32, 32)    -               \nG_synthesis/64x64/Conv0_up  1442816   (?, 256, 64, 64)  (3, 3, 512, 256)\nG_synthesis/64x64/Conv1     852992    (?, 256, 64, 64)  (3, 3, 256, 256)\nG_synthesis/ToRGB_lod0      771       (?, 3, 64, 64)    (1, 1, 256, 3)  \nG_synthesis/Upscale2D_3     -         (?, 3, 64, 64)    -               \nG_synthesis/Grow_lod0       -         (?, 3, 64, 64)    -               \nG_synthesis/images_out      -         (?, 3, 64, 64)    -               \nG_synthesis/lod             -         ()                -               \nG_synthesis/noise0          -         (1, 1, 4, 4)      -               \nG_synthesis/noise1          -         (1, 1, 4, 4)      -               \nG_synthesis/noise2          -         (1, 1, 8, 8)      -               \nG_synthesis/noise3          -         (1, 1, 8, 8)      -               \nG_synthesis/noise4          -         (1, 1, 16, 16)    -               \nG_synthesis/noise5          -         (1, 1, 16, 16)    -               \nG_synthesis/noise6          -         (1, 1, 32, 32)    -               \nG_synthesis/noise7          -         (1, 1, 32, 32)    -               \nG_synthesis/noise8          -         (1, 1, 64, 64)    -               \nG_synthesis/noise9          -         (1, 1, 64, 64)    -               \nimages_out                  -         (?, 3, 64, 64)    -               \n---                         ---       ---               ---             \nTotal                       25137935                                    \n\n\nD                    Params    OutputShape       WeightShape     \n---                  ---       ---               ---             \nimages_in            -         (?, 3, 64, 64)    -               \nlabels_in            -         (?, 0)            -               \nlod                  -         ()                -               \nFromRGB_lod0         1024      (?, 256, 64, 64)  (1, 1, 3, 256)  \n64x64/Conv0          590080    (?, 256, 64, 64)  (3, 3, 256, 256)\n64x64/Conv1_down     1180160   (?, 512, 32, 32)  (3, 3, 256, 512)\nDownscale2D          -         (?, 3, 32, 32)    -               \nFromRGB_lod1         2048      (?, 512, 32, 32)  (1, 1, 3, 512)  \nGrow_lod0            -         (?, 512, 32, 32)  -               \n32x32/Conv0          2359808   (?, 512, 32, 32)  (3, 3, 512, 512)\n32x32/Conv1_down     2359808   (?, 512, 16, 16)  (3, 3, 512, 512)\nDownscale2D_1        -         (?, 3, 16, 16)    -               \nFromRGB_lod2         2048      (?, 512, 16, 16)  (1, 1, 3, 512)  \nGrow_lod1            -         (?, 512, 16, 16)  -               \n16x16/Conv0          2359808   (?, 512, 16, 16)  (3, 3, 512, 512)\n16x16/Conv1_down     2359808   (?, 512, 8, 8)    (3, 3, 512, 512)\nDownscale2D_2        -         (?, 3, 8, 8)      -               \nFromRGB_lod3         2048      (?, 512, 8, 8)    (1, 1, 3, 512)  \nGrow_lod2            -         (?, 512, 8, 8)    -               \n8x8/Conv0            2359808   (?, 512, 8, 8)    (3, 3, 512, 512)\n8x8/Conv1_down       2359808   (?, 512, 4, 4)    (3, 3, 512, 512)\nDownscale2D_3        -         (?, 3, 4, 4)      -               \nFromRGB_lod4         2048      (?, 512, 4, 4)    (1, 1, 3, 512)  \nGrow_lod3            -         (?, 512, 4, 4)    -               \n4x4/MinibatchStddev  -         (?, 513, 4, 4)    -               \n4x4/Conv             2364416   (?, 512, 4, 4)    (3, 3, 513, 512)\n4x4/Dense0           4194816   (?, 512)          (8192, 512)     \n4x4/Dense1           513       (?, 1)            (512, 1)        \nscores_out           -         (?, 1)            -               \n---                  ---       ---               ---             \nTotal                22498049                                    \n\nBuilding TensorFlow graph...\n\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\nIf you depend on functionality not listed there, please file an issue.\n\nSetting up snapshot image grid...\nSetting up run dir...\nTraining...\n\ntick 1     kimg 160.3    lod 4.00  minibatch 128  time 5m 35s       sec/tick 297.2   sec/kimg 1.85    maintenance 38.0   gpumem 1.7 \nnetwork-snapshot-000160        time 16m 22s      fid50k 454.0154  \nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\ntick 2     kimg 320.5    lod 4.00  minibatch 128  time 26m 00s      sec/tick 222.0   sec/kimg 1.39    maintenance 1002.8 gpumem 2.0 \ntick 3     kimg 480.8    lod 4.00  minibatch 128  time 29m 43s      sec/tick 222.0   sec/kimg 1.38    maintenance 1.4    gpumem 2.0 \ntick 4     kimg 620.8    lod 3.97  minibatch 64   time 33m 41s      sec/tick 236.2   sec/kimg 1.69    maintenance 1.2    gpumem 2.0 \ntick 5     kimg 760.8    lod 3.73  minibatch 64   time 41m 24s      sec/tick 462.3   sec/kimg 3.30    maintenance 1.3    gpumem 2.0 \ntick 6     kimg 900.9    lod 3.50  minibatch 64   time 49m 07s      sec/tick 461.2   sec/kimg 3.29    maintenance 1.3    gpumem 2.0 \ntick 7     kimg 1040.9   lod 3.27  minibatch 64   time 56m 49s      sec/tick 461.2   sec/kimg 3.29    maintenance 1.3    gpumem 2.0 \ntick 8     kimg 1180.9   lod 3.03  minibatch 64   time 1h 04m 31s   sec/tick 460.2   sec/kimg 3.29    maintenance 1.3    gpumem 2.0 \ntick 9     kimg 1321.0   lod 3.00  minibatch 64   time 1h 12m 06s   sec/tick 453.5   sec/kimg 3.24    maintenance 1.3    gpumem 2.0 \ntick 10    kimg 1461.0   lod 3.00  minibatch 64   time 1h 19m 40s   sec/tick 452.6   sec/kimg 3.23    maintenance 1.3    gpumem 2.0 \nnetwork-snapshot-001460        time 8m 33s       fid50k 378.7820  \ntick 11    kimg 1601.0   lod 3.00  minibatch 64   time 1h 35m 49s   sec/tick 453.8   sec/kimg 3.24    maintenance 515.6  gpumem 2.0 \ntick 12    kimg 1741.1   lod 3.00  minibatch 64   time 1h 43m 24s   sec/tick 453.8   sec/kimg 3.24    maintenance 1.3    gpumem 2.0 \ntick 13    kimg 1861.1   lod 2.90  minibatch 32   time 1h 57m 38s   sec/tick 852.2   sec/kimg 7.10    maintenance 1.3    gpumem 2.0 \ntick 14    kimg 1981.2   lod 2.70  minibatch 32   time 2h 18m 55s   sec/tick 1275.3  sec/kimg 10.62   maintenance 2.0    gpumem 2.0 \ntick 15    kimg 2101.2   lod 2.50  minibatch 32   time 2h 40m 10s   sec/tick 1273.1  sec/kimg 10.60   maintenance 1.9    gpumem 2.0 \ntick 16    kimg 2221.3   lod 2.30  minibatch 32   time 3h 01m 25s   sec/tick 1273.0  sec/kimg 10.60   maintenance 1.9    gpumem 2.0 \ntick 17    kimg 2341.4   lod 2.10  minibatch 32   time 3h 22m 42s   sec/tick 1275.0  sec/kimg 10.62   maintenance 1.9    gpumem 2.0 \ntick 18    kimg 2461.4   lod 2.00  minibatch 32   time 3h 43m 49s   sec/tick 1265.4  sec/kimg 10.54   maintenance 1.9    gpumem 2.0 \ntick 19    kimg 2581.5   lod 2.00  minibatch 32   time 4h 04m 45s   sec/tick 1253.8  sec/kimg 10.44   maintenance 1.9    gpumem 2.0 \ntick 20    kimg 2701.6   lod 2.00  minibatch 32   time 4h 25m 41s   sec/tick 1254.5  sec/kimg 10.45   maintenance 1.9    gpumem 2.0 \nnetwork-snapshot-002701        time 9m 08s       fid50k 338.4830  \ntick 21    kimg 2821.6   lod 2.00  minibatch 32   time 4h 55m 47s   sec/tick 1255.4  sec/kimg 10.46   maintenance 551.1  gpumem 2.0 \ntick 22    kimg 2941.7   lod 2.00  minibatch 32   time 5h 16m 44s   sec/tick 1254.7  sec/kimg 10.45   maintenance 1.8    gpumem 2.0 \ntick 23    kimg 3041.7   lod 1.93  minibatch 16   time 5h 52m 23s   sec/tick 2136.8  sec/kimg 21.36   maintenance 1.8    gpumem 2.0 \ntick 24    kimg 3141.8   lod 1.76  minibatch 16   time 6h 52m 21s   sec/tick 3593.7  sec/kimg 35.93   maintenance 4.5    gpumem 2.0 \ntick 25    kimg 3241.8   lod 1.60  minibatch 16   time 7h 52m 23s   sec/tick 3597.7  sec/kimg 35.97   maintenance 4.5    gpumem 2.0 \ntick 26    kimg 3341.8   lod 1.43  minibatch 16   time 8h 52m 34s   sec/tick 3606.5  sec/kimg 36.05   maintenance 4.6    gpumem 2.0 \ntick 27    kimg 3400.0   lod 1.33  minibatch 16   time 9h 27m 29s   sec/tick 2090.0  sec/kimg 35.92   maintenance 4.6    gpumem 2.0 \nnetwork-snapshot-003400        time 11m 15s      fid50k 327.9088  \ndnnlib: Finished training.training_loop.training_loop() in 9h 38m 52s.\n\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"text\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e(keras-gpu) C:\\Users\\user\\stylegan-master\u0026gt;python train.py\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\nCreating the run dir: results\\00004-sgan-custom_dataset-1gpu\nCopying files to the run dir\ndnnlib: Running training.training_loop.training_loop() on localhost...\nStreaming data using training.dataset.TFRecordDataset...\nWARNING:tensorflow:From C:\\Users\\user\\stylegan-master\\training\\dataset.py:76: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse eager execution and:\n`tf.data.TFRecordDataset(path)`\nWARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nDataset shape = [3, 128, 128]\nDynamic range = [0, 255]\nLabel size    = 0\nConstructing networks...\n\nG                             Params    OutputShape         WeightShape\n---                           ---       ---                 ---\nlatents_in                    -         (?, 512)            -\nlabels_in                     -         (?, 0)              -\nlod                           -         ()                  -\ndlatent_avg                   -         (512,)              -\nG_mapping/latents_in          -         (?, 512)            -\nG_mapping/labels_in           -         (?, 0)              -\nG_mapping/PixelNorm           -         (?, 512)            -\nG_mapping/Dense0              262656    (?, 512)            (512, 512)\nG_mapping/Dense1              262656    (?, 512)            (512, 512)\nG_mapping/Dense2              262656    (?, 512)            (512, 512)\nG_mapping/Dense3              262656    (?, 512)            (512, 512)\nG_mapping/Dense4              262656    (?, 512)            (512, 512)\nG_mapping/Dense5              262656    (?, 512)            (512, 512)\nG_mapping/Dense6              262656    (?, 512)            (512, 512)\nG_mapping/Dense7              262656    (?, 512)            (512, 512)\nG_mapping/Broadcast           -         (?, 12, 512)        -\nG_mapping/dlatents_out        -         (?, 12, 512)        -\nTruncation                    -         (?, 12, 512)        -\nG_synthesis/dlatents_in       -         (?, 12, 512)        -\nG_synthesis/4x4/Const         534528    (?, 512, 4, 4)      (512,)\nG_synthesis/4x4/Conv          2885632   (?, 512, 4, 4)      (3, 3, 512, 512)\nG_synthesis/ToRGB_lod5        1539      (?, 3, 4, 4)        (1, 1, 512, 3)\nG_synthesis/8x8/Conv0_up      2885632   (?, 512, 8, 8)      (3, 3, 512, 512)\nG_synthesis/8x8/Conv1         2885632   (?, 512, 8, 8)      (3, 3, 512, 512)\nG_synthesis/ToRGB_lod4        1539      (?, 3, 8, 8)        (1, 1, 512, 3)\nG_synthesis/Upscale2D         -         (?, 3, 8, 8)        -\nG_synthesis/Grow_lod4         -         (?, 3, 8, 8)        -\nG_synthesis/16x16/Conv0_up    2885632   (?, 512, 16, 16)    (3, 3, 512, 512)\nG_synthesis/16x16/Conv1       2885632   (?, 512, 16, 16)    (3, 3, 512, 512)\nG_synthesis/ToRGB_lod3        1539      (?, 3, 16, 16)      (1, 1, 512, 3)\nG_synthesis/Upscale2D_1       -         (?, 3, 16, 16)      -\nG_synthesis/Grow_lod3         -         (?, 3, 16, 16)      -\nG_synthesis/32x32/Conv0_up    2885632   (?, 512, 32, 32)    (3, 3, 512, 512)\nG_synthesis/32x32/Conv1       2885632   (?, 512, 32, 32)    (3, 3, 512, 512)\nG_synthesis/ToRGB_lod2        1539      (?, 3, 32, 32)      (1, 1, 512, 3)\nG_synthesis/Upscale2D_2       -         (?, 3, 32, 32)      -\nG_synthesis/Grow_lod2         -         (?, 3, 32, 32)      -\nG_synthesis/64x64/Conv0_up    1442816   (?, 256, 64, 64)    (3, 3, 512, 256)\nG_synthesis/64x64/Conv1       852992    (?, 256, 64, 64)    (3, 3, 256, 256)\nG_synthesis/ToRGB_lod1        771       (?, 3, 64, 64)      (1, 1, 256, 3)\nG_synthesis/Upscale2D_3       -         (?, 3, 64, 64)      -\nG_synthesis/Grow_lod1         -         (?, 3, 64, 64)      -\nG_synthesis/128x128/Conv0_up  426496    (?, 128, 128, 128)  (3, 3, 256, 128)\nG_synthesis/128x128/Conv1     279040    (?, 128, 128, 128)  (3, 3, 128, 128)\nG_synthesis/ToRGB_lod0        387       (?, 3, 128, 128)    (1, 1, 128, 3)\nG_synthesis/Upscale2D_4       -         (?, 3, 128, 128)    -\nG_synthesis/Grow_lod0         -         (?, 3, 128, 128)    -\nG_synthesis/images_out        -         (?, 3, 128, 128)    -\nG_synthesis/lod               -         ()                  -\nG_synthesis/noise0            -         (1, 1, 4, 4)        -\nG_synthesis/noise1            -         (1, 1, 4, 4)        -\nG_synthesis/noise2            -         (1, 1, 8, 8)        -\nG_synthesis/noise3            -         (1, 1, 8, 8)        -\nG_synthesis/noise4            -         (1, 1, 16, 16)      -\nG_synthesis/noise5            -         (1, 1, 16, 16)      -\nG_synthesis/noise6            -         (1, 1, 32, 32)      -\nG_synthesis/noise7            -         (1, 1, 32, 32)      -\nG_synthesis/noise8            -         (1, 1, 64, 64)      -\nG_synthesis/noise9            -         (1, 1, 64, 64)      -\nG_synthesis/noise10           -         (1, 1, 128, 128)    -\nG_synthesis/noise11           -         (1, 1, 128, 128)    -\nimages_out                    -         (?, 3, 128, 128)    -\n---                           ---       ---                 ---\nTotal                         25843858\n\n\nD                    Params    OutputShape         WeightShape\n---                  ---       ---                 ---\nimages_in            -         (?, 3, 128, 128)    -\nlabels_in            -         (?, 0)              -\nlod                  -         ()                  -\nFromRGB_lod0         512       (?, 128, 128, 128)  (1, 1, 3, 128)\n128x128/Conv0        147584    (?, 128, 128, 128)  (3, 3, 128, 128)\n128x128/Conv1_down   295168    (?, 256, 64, 64)    (3, 3, 128, 256)\nDownscale2D          -         (?, 3, 64, 64)      -\nFromRGB_lod1         1024      (?, 256, 64, 64)    (1, 1, 3, 256)\nGrow_lod0            -         (?, 256, 64, 64)    -\n64x64/Conv0          590080    (?, 256, 64, 64)    (3, 3, 256, 256)\n64x64/Conv1_down     1180160   (?, 512, 32, 32)    (3, 3, 256, 512)\nDownscale2D_1        -         (?, 3, 32, 32)      -\nFromRGB_lod2         2048      (?, 512, 32, 32)    (1, 1, 3, 512)\nGrow_lod1            -         (?, 512, 32, 32)    -\n32x32/Conv0          2359808   (?, 512, 32, 32)    (3, 3, 512, 512)\n32x32/Conv1_down     2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\nDownscale2D_2        -         (?, 3, 16, 16)      -\nFromRGB_lod3         2048      (?, 512, 16, 16)    (1, 1, 3, 512)\nGrow_lod2            -         (?, 512, 16, 16)    -\n16x16/Conv0          2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n16x16/Conv1_down     2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\nDownscale2D_3        -         (?, 3, 8, 8)        -\nFromRGB_lod4         2048      (?, 512, 8, 8)      (1, 1, 3, 512)\nGrow_lod3            -         (?, 512, 8, 8)      -\n8x8/Conv0            2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n8x8/Conv1_down       2359808   (?, 512, 4, 4)      (3, 3, 512, 512)\nDownscale2D_4        -         (?, 3, 4, 4)        -\nFromRGB_lod5         2048      (?, 512, 4, 4)      (1, 1, 3, 512)\nGrow_lod4            -         (?, 512, 4, 4)      -\n4x4/MinibatchStddev  -         (?, 513, 4, 4)      -\n4x4/Conv             2364416   (?, 512, 4, 4)      (3, 3, 513, 512)\n4x4/Dense0           4194816   (?, 512)            (8192, 512)\n4x4/Dense1           513       (?, 1)              (512, 1)\nscores_out           -         (?, 1)              -\n---                  ---       ---                 ---\nTotal                22941313\n\nBuilding TensorFlow graph...\nWARNING:tensorflow:From C:\\Users\\user\\stylegan-master\\training\\training_loop.py:167: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nDeprecated in favor of operator or tf.math.divide.\nWARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\nIf you depend on functionality not listed there, please file an issue.\n\nSetting up snapshot image grid...\n2020-01-20 07:05:17.296825: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n2020-01-20 07:05:17.320746: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n2020-01-20 07:05:17.342289: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.15GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n2020-01-20 07:05:17.350675: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n2020-01-20 07:05:17.399302: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.10GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\nSetting up run dir...\nTraining...\n\n2020-01-20 07:05:35.259782: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n2020-01-20 07:05:35.316821: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n2020-01-20 07:05:35.386177: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n2020-01-20 07:05:35.430917: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n2020-01-20 07:05:35.476293: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\ntick 1     kimg 140.0    lod 4.00  minibatch 64   time 8m 55s       sec/tick 483.6   sec/kimg 3.45    maintenance 51.7   gpumem 1.6\nnetwork-snapshot-000140        time 8m 46s       fid50k 360.7307\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\nC:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\ntick 2     kimg 280.1    lod 4.00  minibatch 64   time 25m 58s      sec/tick 479.1   sec/kimg 3.42    maintenance 543.3  gpumem 2.0\ntick 3     kimg 420.1    lod 4.00  minibatch 64   time 33m 59s      sec/tick 479.0   sec/kimg 3.42    maintenance 2.3    gpumem 2.0\ntick 4     kimg 560.1    lod 4.00  minibatch 64   time 42m 01s      sec/tick 479.8   sec/kimg 3.43    maintenance 2.2    gpumem 2.0\ntick 5     kimg 680.2    lod 3.87  minibatch 32   time 59m 14s      sec/tick 1030.6  sec/kimg 8.58    maintenance 2.2    gpumem 2.0\ntick 6     kimg 800.3    lod 3.67  minibatch 32   time 1h 21m 24s   sec/tick 1327.7  sec/kimg 11.06   maintenance 2.2    gpumem 2.0\ntick 7     kimg 920.3    lod 3.47  minibatch 32   time 1h 43m 29s   sec/tick 1323.3  sec/kimg 11.02   maintenance 2.2    gpumem 2.0\ntick 8     kimg 1040.4   lod 3.27  minibatch 32   time 2h 05m 23s   sec/tick 1311.2  sec/kimg 10.92   maintenance 2.2    gpumem 2.0\ntick 9     kimg 1160.4   lod 3.07  minibatch 32   time 2h 27m 16s   sec/tick 1311.7  sec/kimg 10.92   maintenance 2.2    gpumem 2.0\ntick 10    kimg 1280.5   lod 3.00  minibatch 32   time 2h 48m 55s   sec/tick 1296.9  sec/kimg 10.80   maintenance 2.2    gpumem 2.0\nnetwork-snapshot-001280        time 9m 16s       fid50k 292.2210\ntick 11    kimg 1400.6   lod 3.00  minibatch 32   time 3h 19m 47s   sec/tick 1291.2  sec/kimg 10.75   maintenance 560.0  gpumem 2.0\ntick 12    kimg 1520.6   lod 3.00  minibatch 32   time 3h 41m 20s   sec/tick 1291.5  sec/kimg 10.76   maintenance 2.2    gpumem 2.0\ntick 13    kimg 1640.7   lod 3.00  minibatch 32   time 4h 02m 53s   sec/tick 1290.3  sec/kimg 10.75   maintenance 2.3    gpumem 2.0\ntick 14    kimg 1760.8   lod 3.00  minibatch 32   time 4h 24m 26s   sec/tick 1290.8  sec/kimg 10.75   maintenance 2.2    gpumem 2.0\ntick 15    kimg 1860.8   lod 2.90  minibatch 16   time 5h 08m 55s   sec/tick 2667.1  sec/kimg 26.66   maintenance 2.2    gpumem 2.0\ntick 16    kimg 1960.8   lod 2.73  minibatch 16   time 6h 10m 02s   sec/tick 3663.8  sec/kimg 36.63   maintenance 3.3    gpumem 2.0\ntick 17    kimg 2060.9   lod 2.57  minibatch 16   time 7h 11m 09s   sec/tick 3663.3  sec/kimg 36.62   maintenance 3.3    gpumem 2.0\ntick 18    kimg 2160.9   lod 2.40  minibatch 16   time 8h 12m 15s   sec/tick 3663.3  sec/kimg 36.62   maintenance 3.3    gpumem 2.0\ntick 19    kimg 2260.9   lod 2.23  minibatch 16   time 9h 13m 22s   sec/tick 3663.0  sec/kimg 36.62   maintenance 3.3    gpumem 2.0\ntick 20    kimg 2361.0   lod 2.07  minibatch 16   time 10h 14m 28s  sec/tick 3662.6  sec/kimg 36.61   maintenance 3.3    gpumem 2.0\nnetwork-snapshot-002360        time 11m 20s      fid50k 329.8881\ntick 21    kimg 2461.0   lod 2.00  minibatch 16   time 11h 26m 28s  sec/tick 3635.4  sec/kimg 36.34   maintenance 685.2  gpumem 2.0\ntick 22    kimg 2561.0   lod 2.00  minibatch 16   time 12h 27m 40s  sec/tick 3668.3  sec/kimg 36.67   maintenance 3.3    gpumem 2.0\ntick 23    kimg 2661.1   lod 2.00  minibatch 16   time 13h 28m 13s  sec/tick 3630.0  sec/kimg 36.29   maintenance 3.4    gpumem 2.0\ntick 24    kimg 2761.1   lod 2.00  minibatch 16   time 14h 29m 10s  sec/tick 3652.9  sec/kimg 36.52   maintenance 3.4    gpumem 2.0\ntick 25    kimg 2861.1   lod 2.00  minibatch 16   time 15h 29m 52s  sec/tick 3639.3  sec/kimg 36.38   maintenance 3.3    gpumem 2.0\ntick 26    kimg 2961.2   lod 2.00  minibatch 16   time 16h 30m 13s  sec/tick 3617.6  sec/kimg 36.16   maintenance 3.3    gpumem 2.0\ntick 27    kimg 3041.2   lod 1.93  minibatch 8    time 18h 07m 10s  sec/tick 5814.1  sec/kimg 72.68   maintenance 3.3    gpumem 2.0\ntick 28    kimg 3121.2   lod 1.80  minibatch 8    time 20h 29m 23s  sec/tick 8525.3  sec/kimg 106.57  maintenance 7.0    gpumem 2.0\ntick 29    kimg 3201.2   lod 1.66  minibatch 8    time 22h 51m 39s  sec/tick 8528.9  sec/kimg 106.61  maintenance 7.2    gpumem 2.0\ntick 30    kimg 3281.2   lod 1.53  minibatch 8    time 1d 01h 14m   sec/tick 8536.7  sec/kimg 106.71  maintenance 7.3    gpumem 2.0\nnetwork-snapshot-003281        time 14m 53s      fid50k 321.2979\ntick 31    kimg 3361.2   lod 1.40  minibatch 8    time 1d 03h 51m   sec/tick 8535.0  sec/kimg 106.69  maintenance 902.6  gpumem 2.0\ntick 32    kimg 3441.2   lod 1.26  minibatch 8    time 1d 06h 13m   sec/tick 8542.2  sec/kimg 106.78  maintenance 7.4    gpumem 2.0\ntick 33    kimg 3521.2   lod 1.13  minibatch 8    time 1d 08h 36m   sec/tick 8540.9  sec/kimg 106.76  maintenance 7.6    gpumem 2.0\ntick 34    kimg 3601.2   lod 1.00  minibatch 8    time 1d 10h 58m   sec/tick 8538.5  sec/kimg 106.73  maintenance 7.5    gpumem 2.0\ntick 35    kimg 3681.2   lod 1.00  minibatch 8    time 1d 13h 19m   sec/tick 8427.5  sec/kimg 105.34  maintenance 7.5    gpumem 2.0\n．．．\n\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n","createdAt":"2020-01-23T14:28:45Z","elapsedYearsFromUpdatedAt":0,"isBanned":false,"isDeprecated":false,"isDestroyableByViewer":false,"isEditRequestSendableByViewer":true,"isLikableByViewer":true,"isLikedByViewer":false,"isPublic":true,"isSlide":false,"isStockableByViewer":true,"isStockedByViewer":false,"isUpdatableByViewer":false,"isSubscribableByViewer":true,"isSubscribedByViewer":false,"isUpdated":true,"likesCount":2,"originalId":1144200,"title":"【StyleGAN入門】自前マシンでアニメの独自学習♬ ","toc":"\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#%E3%82%84%E3%81%A3%E3%81%9F%E3%81%93%E3%81%A8\"\u003eやったこと\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E3%82%A2%E3%83%8B%E3%83%A1%E9%A1%94%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E6%BA%96%E5%82%99\"\u003e・アニメ顔データの準備\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E3%81%A8%E3%81%AB%E3%81%8B%E3%81%8F%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B\"\u003e・とにかく学習する\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E6%BD%9C%E5%9C%A8%E7%A9%BA%E9%96%93%E3%81%A7%E3%81%AE%E3%83%9F%E3%82%AD%E3%82%B7%E3%83%B3%E3%82%B0%E3%82%92%E3%82%84%E3%81%A3%E3%81%A6%E3%81%BF%E3%82%8B\"\u003e・潜在空間でのミキシングをやってみる\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E5%86%8D%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B%E3%81%AB%E3%81%AF\"\u003e・再学習するには\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E3%81%BE%E3%81%A8%E3%82%81\"\u003eまとめ\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E3%81%8A%E3%81%BE%E3%81%91\"\u003eおまけ\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n","totalPv":1024,"updatedAt":"2020-01-23T22:36:56Z","uuid":"aec7feabaa2f738ea82c","banReason":null,"adventCalendarItem":null,"author":{"originalId":233744,"description":"2020年目標；いい記事を書く\r\n\r\n記事350いいね1500フォロワー150\r\n2019年の実績／目標\r\n記事275／300いいね1035／1000フォロワー97／100\r\n1/7/2019\r\n記事219いいね784フォロワー76\r\n2018年の実績／目標\r\n記事140／200いいね423／500フォロワー48／50\r\n7/8/2018\r\n記事90いいね227フォロワー25","name":"ムー ウワン","profileImageUrl":"https://qiita-user-profile-images.imgix.net/https%3A%2F%2Favatars2.githubusercontent.com%2Fu%2F35788955%3Fv%3D4?ixlib=rb-1.2.2\u0026auto=compress%2Cformat\u0026lossless=0\u0026w=48\u0026s=c7c7473101694938ee68dd578e6ab883","profileImageUrlW48":"https://qiita-user-profile-images.imgix.net/https%3A%2F%2Favatars2.githubusercontent.com%2Fu%2F35788955%3Fv%3D4?ixlib=rb-1.2.2\u0026auto=compress%2Cformat\u0026lossless=0\u0026w=48\u0026s=c7c7473101694938ee68dd578e6ab883","profileImageUrlW75":"https://qiita-user-profile-images.imgix.net/https%3A%2F%2Favatars2.githubusercontent.com%2Fu%2F35788955%3Fv%3D4?ixlib=rb-1.2.2\u0026auto=compress%2Cformat\u0026lossless=0\u0026w=75\u0026s=e420ddb9abfbecae939e655562343f59","urlName":"MuAuan","isBlockingViewer":false,"isFollowedByViewer":false,"isFollowableByViewer":true,"websiteUrl":"","organizations":{"edges":[]}},"tags":[{"name":"Python","urlName":"python"},{"name":"機械学習","urlName":"%e6%a9%9f%e6%a2%b0%e5%ad%a6%e7%bf%92"},{"name":"DeepLearning","urlName":"deeplearning"},{"name":"動画","urlName":"%e5%8b%95%e7%94%bb"},{"name":"stylegan","urlName":"stylegan"}],"followingLikers":{"edges":[]},"comments":{"totalCount":0}},"viewer":{"isGithubLoginEnabled":false,"contribution":0,"numberOfDaysSinceRegistration":59,"organizations":{"edges":[]},"followers":{"totalCount":0},"articles":{"totalCount":0},"tags":{"edges":[{"node":{"name":"MacOSX"}},{"node":{"name":"Xcode"}},{"node":{"name":"Chrome"}},{"node":{"name":"GitHub"}}]},"postingArticleTags":{"edges":[]}},"analyticsTrackingId":null}</script>
      
<script src="//d-cache.microad.jp/js/td_qt_access.js" type="text/javascript"></script><script>microadTd.QT.start({"article_category": "Python,機械学習,DeepLearning,動画,stylegan"})</script><script>(function(d,u){var b=d.getElementsByTagName("script")[0],j=d.createElement("script");j.async=true;j.src=u;b.parentNode.insertBefore(j,b);})(document,"//img.ak.impact-ad.jp/ut/ff9a3577423c8ed5_4330.js");</script><noscript><iframe frameborder="0" height="0" src="//nspt.unitag.jp/ff9a3577423c8ed5_4330.php" width="0"></iframe></noscript><script async="" src="https://www.googletagmanager.com/gtag/js?id=AW-878053044"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'AW-878053044');</script><script>!function(f,b,e,v,n,t,s)
{if(f.fbq)return;n=f.fbq=function(){n.callMethod?
n.callMethod.apply(n,arguments):n.queue.push(arguments)};
if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
n.queue=[];t=b.createElement(e);t.async=!0;
t.src=v;s=b.getElementsByTagName(e)[0];
s.parentNode.insertBefore(t,s)}(window, document,'script',
'https://connect.facebook.net/en_US/fbevents.js');
fbq('init', '1792588824374455');
fbq('track', 'PageView');</script><noscript><img height="1" src="https://www.facebook.com/tr?id=1792588824374455&amp;ev=PageView&amp;noscript=1" style="display:none" width="1" /></noscript><footer id="globalFooter" class="st-Footer"><div class="st-Footer_container"><div class="st-Footer_start"><div class="st-Footer_logo"><svg viewbox="0 0 426.57 130" xmlns="http://www.w3.org/2000/svg"><circle cx="167.08" cy="21.4" r="12.28" /><path d="M250.81 29.66h23.48v18.9h-23.48z" /><path d="M300.76 105.26a22.23 22.23 0 01-6.26-.86 12.68 12.68 0 01-5.17-3 14.41 14.41 0 01-3.56-5.76 28 28 0 01-1.3-9.22V48.56h29.61v-18.9h-29.52V3.29h-20.17v83.34q0 11.16 2.83 18.27a27.71 27.71 0 007.7 11.2 26.86 26.86 0 0011.43 5.62 47.56 47.56 0 0012.34 1.53h15.16v-18zM0 61.7a58.6 58.6 0 015-24.21A62.26 62.26 0 0118.73 17.9 63.72 63.72 0 0139 4.78 64.93 64.93 0 0164 0a65 65 0 0124.85 4.78 64.24 64.24 0 0120.38 13.12A62 62 0 01123 37.49a58.6 58.6 0 015 24.21 58.34 58.34 0 01-4 21.46 62.8 62.8 0 01-10.91 18.16l11.1 11.1a10.3 10.3 0 010 14.52 10.29 10.29 0 01-14.64 0l-12.22-12.41a65 65 0 01-15.78 6.65 66.32 66.32 0 01-17.55 2.3 64.63 64.63 0 01-45.23-18A62.82 62.82 0 015 85.81 58.3 58.3 0 010 61.7zm21.64.08a43.13 43.13 0 0012.42 30.63 42.23 42.23 0 0013.43 9.09A41.31 41.31 0 0064 104.8a42 42 0 0030-12.39 42.37 42.37 0 009-13.64 43.43 43.43 0 003.3-17 43.77 43.77 0 00-3.3-17A41.7 41.7 0 0080.55 22 41.78 41.78 0 0064 18.68 41.31 41.31 0 0047.49 22a42.37 42.37 0 00-13.43 9.08 43.37 43.37 0 00-12.42 30.7zM331.89 78a47.59 47.59 0 013.3-17.73 43.22 43.22 0 019.34-14.47A44.25 44.25 0 01359 36a47.82 47.82 0 0118.81-3.58 42.72 42.72 0 019.26 1 46.5 46.5 0 018.22 2.58 40 40 0 017 3.84 44.39 44.39 0 015.71 4.63l1.22-9.47h17.35v85.83h-17.35l-1.17-9.42a42.54 42.54 0 01-5.84 4.67 43.11 43.11 0 01-7 3.79 44.86 44.86 0 01-8.17 2.59 43 43 0 01-9.22 1A47.94 47.94 0 01359 119.9a43.3 43.3 0 01-14.47-9.71 44.17 44.17 0 01-9.34-14.47 47 47 0 01-3.3-17.72zm20.27-.08a29.16 29.16 0 002.17 11.34 27 27 0 005.92 8.88 26.69 26.69 0 008.76 5.76 29.19 29.19 0 0021.44 0 26.11 26.11 0 008.72-5.76 27.57 27.57 0 005.88-8.84 29 29 0 002.16-11.38 28.62 28.62 0 00-2.16-11.22 26.57 26.57 0 00-5.93-8.8 27.68 27.68 0 00-19.51-7.9 28.29 28.29 0 00-10.77 2.05 26.19 26.19 0 00-8.71 5.75 27.08 27.08 0 00-5.84 8.8 28.94 28.94 0 00-2.13 11.31zm-194.97-30.5h19.78v73.54h-19.78zm49.25 0h19.78v73.54h-19.78z" /><circle cx="216.33" cy="21.4" r="12.28" /></svg></div><div class="st-Footer_catchcopy">How developers code is here.</div><div class="st-Footer_socials"><a class="fa fa-twitter" href="https://twitter.com/qiita"></a><a class="fa fa-facebook-square" href="https://www.facebook.com/qiita/"></a></div></div><div class="st-Footer_end"><div class="st-Footer_qiita"><div class="st-Footer_label">Qiita</div><div class="st-Footer_list"><div class="st-Footer_column"><a href="/about">About</a><a href="/terms">利用規約</a><a href="/privacy">プライバシー</a><a target="_blank" href="http://help.qiita.com/ja/articles/qiita-community-guideline">ガイドライン</a></div><div class="st-Footer_column"><a href="/api/v2/docs">API</a><a href="/feedback/new">ご意見</a><a href="https://help.qiita.com">ヘルプ</a><a target="_blank" href="https://qiita.com/ads?utm_source=qiita&amp;utm_medium=referral&amp;utm_content=footer">広告掲載</a></div></div></div><div class="st-Footer_increments"><div class="st-Footer_label">Increments</div><div class="st-Footer_list"><div class="st-Footer_column"><a href="https://increments.co.jp/company/">About</a><a href="https://increments.co.jp/jobs/">採用情報</a><a href="https://blog.qiita.com">ブログ</a></div><div class="st-Footer_column"><a href="https://teams.qiita.com/">Qiita Team</a><a href="https://jobs.qiita.com?utm_source=qiita&amp;utm_medium=referral&amp;utm_content=footer">Qiita Jobs</a><a href="https://zine.qiita.com?utm_source=qiita&amp;utm_medium=referral&amp;utm_content=footer">Qiita Zine</a></div></div></div></div></div><div class="st-Footer_copyright">© 2011-2020 Increments Inc.</div></footer><div id="Snackbar-react-component-d4549c2d-7f9b-4cd8-9bbb-d19172bc0e45"></div>
      <script type="application/json" class="js-react-on-rails-component" data-component-name="Snackbar" data-dom-id="Snackbar-react-component-d4549c2d-7f9b-4cd8-9bbb-d19172bc0e45">{}</script>
      
<div id="LoginModal-react-component-dfc47b5a-96ae-43a5-b390-5a7204c09d8c"><div class="st-Modal"><div class="st-Modal_backdrop"></div><div class="st-Modal_body lm-Dialog"><div class="fa fa-times lm-Dialog_close"></div><div class="lm-Dialog_title">ユーザー登録して、Qiitaをもっと便利に使ってみませんか。</div><div class="lm-Dialog_message-pc">この機能を利用するにはログインする必要があります。ログインするとさらに便利にQiitaを利用できます。</div><div class="lm-Dialog_message-mobile">この機能を利用するにはログインする必要があります。ログインするとさらに便利にQiitaを利用できます。</div><ol class="lm-Dialog_list"><li>あなたにマッチした記事をお届けします<div class="description">ユーザーやタグをフォローすることで、あなたが興味を持つ技術分野の情報をまとめてキャッチアップできます</div></li><li>便利な情報をあとで効率的に読み返せます<div class="description">気に入った記事を「ストック」することで、あとからすぐに検索できます</div></li></ol><a href="/signup?callback_action=login_or_signup&amp;redirect_to=%2FMuAuan%2Fitems%2Faec7feabaa2f738ea82c&amp;realm=qiita" class="lm-Dialog_button lm-Dialog_button-signup">登録する</a><a href="/login?callback_action=login_or_signup&amp;redirect_to=%2FMuAuan%2Fitems%2Faec7feabaa2f738ea82c&amp;realm=qiita" class="lm-Dialog_button lm-Dialog_button-signin">ログインする</a></div></div></div>
      <script type="application/json" class="js-react-on-rails-component" data-component-name="LoginModal" data-dom-id="LoginModal-react-component-dfc47b5a-96ae-43a5-b390-5a7204c09d8c">{}</script>
      
<div id="UserHoverCard-react-component-478cdb6e-23cb-4103-bede-ef42a5fa7b72"><div class="uh-Container" style="left:0px;top:0px"><div>ユーザーは見つかりませんでした</div></div></div>
      <script type="application/json" class="js-react-on-rails-component" data-component-name="UserHoverCard" data-dom-id="UserHoverCard-react-component-478cdb6e-23cb-4103-bede-ef42a5fa7b72">{}</script>
      
</div><div id="dataContainer" style="display: none;" data-config="{&quot;actionPath&quot;:&quot;public/items#show&quot;,&quot;settings&quot;:{&quot;analyticsTrackingId&quot;:&quot;UA-24675221-12&quot;,&quot;mixpanelToken&quot;:&quot;17d24b448ca579c365d2d1057f3a1791&quot;,&quot;assetsMap&quot;:{},&quot;csrfToken&quot;:&quot;ASpPv2F6Mthg0dGa2Fe61xztsZTznXFCFRTDOvRN3nHYXDSo33/HQU1zrz2LP7L2bNbfc/dz8HH3IUmqphUkHg==&quot;,&quot;locale&quot;:&quot;ja&quot;},&quot;currentUser&quot;:{&quot;isStaff&quot;:false,&quot;isJobseeker&quot;:false,&quot;name&quot;:&quot;&quot;,&quot;originalId&quot;:595703,&quot;profileImageUrl&quot;:&quot;https://qiita-user-profile-images.imgix.net/https%3A%2F%2Flh6.googleusercontent.com%2F-3EHSHCCSCmg%2FAAAAAAAAAAI%2FAAAAAAAAAAA%2FAKF05nAVp3wix0GPOS2jqZ7xSDRYCDF6Mw%2Fs50%2Fphoto.jpg?ixlib=rb-1.2.2\u0026auto=compress%2Cformat\u0026lossless=0\u0026w=48\u0026s=be4e5cacc9e0864ac1ef96d37914a7b9&quot;,&quot;urlName&quot;:&quot;syoborian&quot;,&quot;remainingPublicImageUploadableSizeInCurrentMonth&quot;:104857600,&quot;monthlyPublicImageUploadableSizeLimit&quot;:104857600}}" /></body></html><script type="application/json" data-js-react-on-rails-store="AppStore">{"snackbar":{"type":"","body":"","isActive":false}}</script>